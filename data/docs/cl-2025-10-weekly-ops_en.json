{
  "doc_key": "cl-2025-10-weekly-ops",
  "title": "Checklist CL-2025-10: Weekly Operations Checklist",
  "doc_type": "checklist",
  "created_at": "2025-10-06",
  "content": "# Checklist CL-2025-10: Weekly Operations Checklist (Platform + Core Services)\n\n## Purpose\nA weekly checklist to reduce “slow burn” outages: disk filling, queue backlogs, retention misconfig, expired secrets, and creeping DB load. This is not incident response; it’s preventative maintenance.\n\nTime budget: ~45–60 minutes. Run it early week (Mon/Tue) during business hours.\n\n## 1) Postgres health\n### Connections and slow queries\n- Check `numbackends` during peak.\n- Confirm no growing long-running queries.\n\nSQL snippets:\n```sql\nselect count(*) from pg_stat_activity;\nselect pid, now()-query_start as age, state, query\nfrom pg_stat_activity\nwhere state <> 'idle'\norder by age desc\nlimit 10;\n```\n\n### Vacuum / bloat\n- Identify tables with high dead tuples (common: `cart_items`, `stripe_events`).\nIf bloat is rising, schedule maintenance; don’t “vacuum full” during peak.\n\n## 2) PgBouncer sanity\n- Confirm services are connecting via PgBouncer (application_name patterns).\n- Review `cl_waiting` and `sv_active` for saturation signs.\n\nIf you see many direct connections to primary from app pods, open a ticket immediately; it is a common precursor to RB-2024-11 incidents.\n\n## 3) Redis: memory and eviction watch\n- Check maxmemory usage and eviction rate.\n- Review cache value size histograms for cart-service.\n\nIf `redis_evicted_keys_total` has sustained increases during normal traffic, investigate:\n- oversized values (recommendations accidentally cached)\n- TTL synchronization (missing jitter)\n- sudden traffic surges\n\nNote: Redis issues have caused checkout instability before (PM-2025-11).\n\n## 4) Elasticsearch retention and health\n- Cluster health should be green.\n- Disk usage should remain below low watermark.\n\nCommands:\n```bash\ncurl -s http://elasticsearch:9200/_cluster/health?pretty\ncurl -s http://elasticsearch:9200/_cat/indices/products-v3-*?h=index,store.size,docs.count&s=index\ncurl -s http://elasticsearch:9200/_cat/aliases/products_read?v\n```\n\nIf there are indices older than 14 days that still exist, ILM is likely broken. Create a ticket; don’t wait for disk watermark incidents.\n\n## 5) Stripe webhook hygiene\n- Check `stripe_webhook_queue_depth` baseline.\n- Review signature failure counts (`stripe_signature_fail_total`).\n\nIf signature failures are non-zero in prod:\n- verify no partial rollout of payment-gateway secrets\n- ensure ingress path for `/webhooks/stripe` hasn’t changed\n\n## 6) Background jobs and queues\n- indexing-worker Kafka lag should not be growing week-over-week.\n- refund-reconciler should have completed successfully in the last 24h.\n- stripe replay cronjob should be idle (manual runs are rare).\n\nIf refund-reconciler failed, support will notice days later; fix now.\n\n## 7) Secrets and certificates\n- Confirm Stripe keys are not near rotation deadline (quarterly).\n- Check ingress TLS cert expiry (our automation usually handles it, but verify).\n\nIf a secret was rotated, ensure all pods rolled; partial rotation causes intermittent failures that are hard to debug.\n\n## 8) Dashboards and alert noise\n- Review “top noisy alerts” and tune thresholds if they are flapping.\n- Ensure oncall dashboards still load quickly and panels point to valid metrics.\n\n## 9) Small housekeeping\n- Remove unused feature flags that are permanently on/off.\n- Confirm `READINESS_STRICT_DEPS` settings are consistent with current resilience strategy.\n\n## Output\nFile a weekly ops ticket with:\n- any anomalies\n- tickets opened\n- screenshots/links to dashboards if relevant\n\n## References\n- Runbooks: Redis evictions, Stripe webhook backlog, ES cluster red, Postgres connection exhaustion\n- Checklists: Production deploy\n\n## 10) CI/CD and registry hygiene\n- Verify GitHub Actions runners are healthy (no sustained queue of jobs).\n- Check that release images are being built and pushed (we've had “ImagePullBackOff” incidents from missing tags).\n- Confirm registry retention is deleting old sha tags to avoid storage creep.\n\nQuick sanity:\n- pick the last promoted SHA and confirm image exists in registry\n- verify `promote.yml` workflow success rate in the last week\n\n## 11) SLO / error budget review\n- Review weekly error budget burn for checkout-service and payment-gateway.\n- If burn > 30% in a week, open a reliability ticket even if no single SEV incident occurred.\nThis catches “death by a thousand cuts” (timeouts, small 5xx spikes).\n\n## 12) One-off checks (only if relevant)\n- After any Redis config change, confirm eviction policy remains `allkeys-lfu`.\n- After any ES reindex, confirm old indices were deleted and aliases point correctly.",
  "language": "en"
}