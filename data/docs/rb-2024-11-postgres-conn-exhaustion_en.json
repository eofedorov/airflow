{
  "doc_key": "rb-2024-11-postgres-conn-exhaustion",
  "title": "Runbook: Postgres Connection Exhaustion",
  "doc_type": "runbook",
  "created_at": "2024-11-09",
  "content": "# Summary\nThis runbook is for incidents where Postgres rejects new connections (`FATAL: sorry, too many clients already`) or becomes unstable due to too many concurrent sessions. This typically affects multiple services at once (checkout-service, cart-service, payment-gateway) because they share the same primary Postgres cluster.\n\nWe use PgBouncer in transaction pooling mode (ADR-2024-08). Most incidents come from services accidentally bypassing PgBouncer or increasing pool sizes in a deploy.\n\n# Symptoms\n## User-facing\n- Widespread 500s across cart/checkout\n- Requests hang then fail (timeouts) as pools block waiting for a connection\n\n## Logs\nFastAPI (psycopg/SQLAlchemy):\n- `psycopg.OperationalError: connection failed: FATAL: sorry, too many clients already`\n- `sqlalchemy.exc.TimeoutError: QueuePool limit of size 30 overflow 20 reached`\n\nNode payment-gateway (pg module):\n- `Error: remaining connection slots are reserved for non-replication superuser connections`\n\n## Metrics\n- Postgres `numbackends` near `max_connections` (prod default 600)\n- `pgbouncer_clients_waiting` increases (if PgBouncer is working but saturated)\n- Elevated latency on DB-dependent endpoints\n\n# Architecture notes\n- Primary Postgres endpoint (do not use directly from apps):\n  - `postgres-primary.ecommerce-prod.svc:5432`\n- PgBouncer endpoint (apps should use):\n  - `pgbouncer.ecommerce-prod.svc:6432`\n- Migrations (alembic) should connect to primary directly (not PgBouncer).\n\n# Quick triage\n## 1) Confirm connection counts and top offenders\nConnect as admin (read-only is fine for triage):\n```sql\nshow max_connections;\nselect count(*) as total from pg_stat_activity;\nselect application_name, usename, state, count(*)\nfrom pg_stat_activity\ngroup by 1,2,3\norder by count(*) desc\nlimit 15;\n```\nConventions (application_name):\n- `checkout-service@<pod>`\n- `cart-service@<pod>`\n- `payment-gateway@<pod>`\nIf application_name is blank or looks like a driver default, it may indicate a misconfigured connection string.\n\n## 2) Check for “idle in transaction”\n```sql\nselect pid, application_name, now() - xact_start as xact_age, query\nfrom pg_stat_activity\nwhere state = 'idle in transaction'\norder by xact_age desc\nlimit 20;\n```\nIf there are many long-lived idle-in-tx sessions, they can pin resources and block vacuum.\n\n## 3) Check PgBouncer health\nIf you can connect to PgBouncer admin:\n```sql\nshow pools;\nshow stats;\n```\nLook at:\n- `cl_active`, `cl_waiting`\n- server pool sizes\n\n# Mitigation options\n## Mitigation A: Reduce app concurrency (fastest and safest)\nFor FastAPI services, worker count multiplies DB pools.\n- Temporarily set `WEB_CONCURRENCY=2` (default 4–6 in prod)\n- Lower pool sizes:\n  - `DB_POOL_SIZE=10`\n  - `DB_MAX_OVERFLOW=5`\n```bash\nkubectl -n ecommerce-prod set env deploy/checkout-service WEB_CONCURRENCY=2 DB_POOL_SIZE=10 DB_MAX_OVERFLOW=5\nkubectl -n ecommerce-prod set env deploy/cart-service WEB_CONCURRENCY=2 DB_POOL_SIZE=10 DB_MAX_OVERFLOW=5\n```\nWatch Postgres connections after 2–3 minutes.\n\n## Mitigation B: Force apps back through PgBouncer\nIf a deploy changed `DATABASE_URL` to point at primary:\n1. Roll back or patch env var to PgBouncer:\n- `DATABASE_URL=postgresql+psycopg://app:***@pgbouncer.ecommerce-prod.svc:6432/ecommerce`\n2. Restart pods.\n\nThis is the most common “why did it suddenly happen?” root cause.\n\n## Mitigation C: Kill problematic sessions (last resort)\nOnly do this if you’ve identified a set of sessions that are clearly stuck and not doing useful work. Example: 1000 idle-in-tx sessions from one pod.\n```sql\nselect pg_terminate_backend(pid)\nfrom pg_stat_activity\nwhere application_name like 'checkout-service@%' and state='idle in transaction'\n  and now() - xact_start > interval '2 minutes';\n```\n**Warning:** terminating sessions can cause request failures and partial writes. Prefer to reduce load first.\n\n## Mitigation D: Enable rate limits to protect the DB\nIf the DB is close to collapsing (timeouts everywhere):\n- rate limit `/api/checkout/start` and `/api/cart/*` at ingress\n- prefer 429 over 500 cascades\n- disable non-essential workers (indexing-worker) temporarily\n\n# Recovery\nA good recovery state:\n- total connections < 420 (70% of max)\n- no growing backlog of requests waiting for DB connections\n- PgBouncer `cl_waiting` near 0\n\nAfter stabilization:\n1. Gradually restore `WEB_CONCURRENCY` and pool sizes.\n2. If you changed env vars, create a follow-up PR to fix defaults.\n3. Add a postmortem if incident > 15 minutes or user impact significant.\n\n# Edge cases and nuances\n- **Connection leaks:** missing `session.close()` on exception paths; manifests as slowly increasing connections after deploy.\n- **Async pool misuse:** mixing sync and async engines can double pools.\n- **Prepared statements:** PgBouncer transaction pooling breaks session-scoped prepared statements. If a service silently enables prepared statements, it may behave poorly under pooling.\n- **Background jobs:** refund-reconciler and indexing-worker can create surprising connection load; include them in offender analysis.\n- **Read replicas:** if read traffic is accidentally routed to primary due to DNS misconfig, you’ll see more connections without a deploy.\n\n# References\n- ADR-2024-08 PgBouncer transaction pooling\n- Runbook RB-2025-06 Kubernetes rollout stuck (often related)",
  "language": "en"
}