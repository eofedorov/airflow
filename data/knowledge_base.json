{
  "documents": [
    {
      "doc_id": "rb-2026-02-redis-evictions",
      "title": "Runbook: Redis Evictions and Cart/Checkout Inconsistency",
      "document_type": "runbook",
      "created_at": "2026-02-02",
      "content": "# Summary\nThis runbook covers incidents where Redis cache behavior (evictions, latency, replica lag, or outright unavailability) causes inconsistent cart behavior and checkout failures. The most common pattern is a spike in `409 CART_VERSION_MISMATCH` and `412 PRECONDITION_FAILED` during `/api/checkout/start`, paired with `redis_evicted_keys_total` increasing and `cart-service` cache miss ratio > 0.85.\n\n**Primary services:** cart-service, checkout-service  \n**Primary dependency:** Redis (clustered, Redis 7.2.4)  \n**Source of truth:** Postgres (carts + cart_items)\n\n# What “normal” looks like\n- cart-service p95 latency: 80–140ms\n- checkout-service `http_409_total` near zero\n- Redis:\n  - `used_memory / maxmemory` < 0.75\n  - `evicted_keys` flat (0/min)\n  - `instantaneous_ops_per_sec` stable (no saw-tooth)\n- Cart cache:\n  - average value size 10–40KB\n  - TTL 900s (`CART_CACHE_TTL_SECONDS=900`)\n\n# Symptoms\n## User-facing\n- Cart page “drops” items after refresh (usually from a different session/device)\n- Checkout page loops: “Something changed in your cart. Please retry.”\n- Payment attempts stuck in pending due to cart snapshot mismatch\n\n## Service logs / errors\ncart-service:\n- `redis.exceptions.ConnectionError: Error 111 connecting to redis.ecommerce-prod.svc:6379. Connection refused`\n- `CacheWriteSkipped size_bytes=221332 reason=value_too_large`\n- `cart_etag_source=redis etag_reset=true`\n\ncheckout-service:\n- `CartPreconditionFailed: missing If-Match`\n- `CART_VERSION_MISMATCH expected=18 got=17 cart_id=...`\n\n# Dashboards and alerts\nGrafana folders:\n- **Platform / Redis / Cache Health**\n- **cart-service / Cache + DB**\n- **checkout-service / Funnel**\n\nPrometheus alert rules (names as in `infra/observability/alerts.yml`):\n- `RedisEvictionsHigh`: `increase(redis_evicted_keys_total[5m]) > 500`\n- `CartVersionMismatchSpike`: `rate(http_responses_total{service=\"checkout-service\",status=\"409\"}[5m]) > 5`\n\n# Fast triage (5 minutes)\n1. Confirm incident scope:\n   - Is it only carts/checkout, or are other Redis-backed features failing (rate-limits, Stripe idempotency, sessions)?\n2. Check Redis health:\n   ```bash\n   kubectl -n ecommerce-prod port-forward svc/redis 6379:6379\n   redis-cli -p 6379 INFO memory | egrep 'used_memory_human|maxmemory_human|mem_fragmentation_ratio'\n   redis-cli -p 6379 INFO stats | egrep 'evicted_keys|keyspace_hits|keyspace_misses'\n   redis-cli -p 6379 CLUSTER INFO\n   ```\n3. Check cart cache miss + value size:\n   - cart-service metric: `cart_cache_value_size_bytes{p95}`\n   - If not available, use debug header (only on staging by default):\n     - `X-Debug-Cache-Key: cart:{user_id}:v{version}`\n     - `X-Debug-Cache-Bytes: <int>`\n4. Check recent deploys:\n   - If cart-service was deployed in the last 60 minutes, search the diff for changes to cart serialization, recommendations, or TTL logic.\n\n# Mitigation options\nPick the least risky option that stops user-visible failures.\n\n## Mitigation A: Bypass Redis for cart reads (preferred during evictions)\nEnable a temporary bypass so cart-service reads from Postgres and does not write back.\n- Env:\n  - `CART_CACHE_BYPASS=true`\n  - `CART_CACHE_WRITEBACK=false`\n```bash\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_BYPASS=true CART_CACHE_WRITEBACK=false\nkubectl -n ecommerce-prod rollout status deploy/cart-service\n```\n**Trade-off:** Postgres load increases. Watch `pg_stat_activity` and checkout latency.\n\n## Mitigation B: Reduce cart cache TTL and cap cache payload\nIf Redis is healthy but memory is near max:\n- Reduce TTL: `CART_CACHE_TTL_SECONDS=120`\n- Enforce payload cap: `CART_CACHE_MAX_BYTES=131072` (128KB)\n```bash\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_TTL_SECONDS=120 CART_CACHE_MAX_BYTES=131072\n```\n**Note:** If the cap is enabled, large carts will skip caching; that’s okay. It is better than thrashing Redis.\n\n## Mitigation C: Stabilize ETag derivation\nIf checkout failures are primarily 409s:\n- Switch ETag source from cached payload to Postgres snapshot:\n  - `CART_ETAG_SOURCE=postgres`\n- Additionally allow checkout-service to retry once:\n  - `CHECKOUT_RETRY_ON_CART_MISMATCH=true` (temporary)\n```bash\nkubectl -n ecommerce-prod set env deploy/checkout-service CART_ETAG_SOURCE=postgres CHECKOUT_RETRY_ON_CART_MISMATCH=true\n```\n\n## Mitigation D: Free Redis memory (last resort)\nIf you must free memory quickly:\n1. Identify large keys (slow; do not run during peak unless necessary):\n   ```bash\n   redis-cli -p 6379 --scan --pattern 'cart:*' | head -n 2000 | \\\n     xargs -n 50 redis-cli -p 6379 MEMORY USAGE\n   ```\n2. Delete only the largest offenders (usually carts with embedded metadata):\n   ```bash\n   redis-cli -p 6379 --scan --pattern 'cart:*' | head -n 500 | \\\n     xargs -n 50 redis-cli -p 6379 DEL\n   ```\n**Warning:** This can amplify DB load and worsen checkout for a few minutes.\n\n# Recovery and verification\n1. Metrics should stabilize:\n   - `redis_evicted_keys_total` flat\n   - cart-service cache miss ratio < 0.5 (after re-enable)\n   - checkout-service 409 rate returns to baseline\n2. Re-enable cache gradually:\n   - First allow reads (writes disabled), then writeback:\n```bash\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_BYPASS=false CART_CACHE_WRITEBACK=false\n# wait 10–15 minutes\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_WRITEBACK=true CART_CACHE_TTL_SECONDS=900\n```\n3. Validate with a real flow (staging if possible):\n   - Add item, remove item, checkout start, ensure totals match.\n\n# Edge cases and “gotchas”\n- **Replica lag:** if Redis reads are served from replicas and replication lags, ETag can regress. Symptom: “version goes backwards.” Confirm `role:slave` INFO replication and `master_link_status=up`. If lag > 1s, force reads from masters temporarily.\n- **Key format mismatch after deploy:** if key prefix changed (e.g., `cart2:` vs `cart:`), you’ll see near-100% misses but no evictions. Roll back or add compatibility read path.\n- **Large carts (>200 items):** serialization can exceed 256KB and trigger timeouts. cart-service should log `CacheWriteSkipped reason=value_too_large`.\n- **Thundering herd on rehydrate:** after bypass ends, thousands of carts repopulate at once. Consider jittered TTL (`CART_CACHE_TTL_JITTER_SECONDS=120`) to avoid synchronized expirations.\n- **Ingress retries:** if the client or gateway retries `POST /api/cart/.../items` without idempotency keys, duplicate increments happen. Verify `Idempotency-Key` usage in clients.\n\n# References\n- ADR-2025-03 Redis as primary cache for carts\n- PM-2025-11 Redis OOM and evictions broke checkout consistency\n- Checklist CL-2025-09 Production deploy (watch Redis evictions on cache changes)"
    },
    {
      "doc_id": "rb-2026-01-stripe-webhook-backlog",
      "title": "Runbook: Stripe Webhook Backlog and Payment Pending Orders",
      "document_type": "runbook",
      "created_at": "2026-01-14",
      "content": "# Summary\nThis runbook is for incidents where Stripe webhooks stop being processed quickly enough, causing orders to remain in `PAYMENT_PENDING`, refunds to stall, or duplicate fulfillment attempts. Our authoritative payment state comes from webhooks (see ADR-2024-12). When webhooks fail, **do not** “fix” by polling Stripe in the hot path; that usually increases API errors and makes recovery harder.\n\n**Primary services:** payment-gateway (Node), checkout-service (FastAPI)  \n**Dependencies:** Stripe, Postgres, Redis, ingress (raw body handling)\n\n# Symptoms\n## User-facing\n- “Payment pending” for > 2 minutes after card confirmation\n- Customer support sees PaymentIntent succeeded in Stripe dashboard, but our order is still pending\n- Refund button says “Requested” and never moves to “Completed”\n\n## Metrics and alerts\nPrometheus metrics emitted by payment-gateway:\n- `stripe_webhook_requests_total{status}`\n- `stripe_webhook_queue_depth`\n- `stripe_webhook_process_seconds_bucket`\n- `stripe_signature_fail_total`\n\nAlerts (common):\n- `StripeWebhookQueueHigh`: `stripe_webhook_queue_depth > 2000 for 10m`\n- `StripeWebhookSignatureFailures`: `rate(stripe_signature_fail_total[5m]) > 1`\n\n# Known architecture (current)\n1. Stripe delivers webhook to `POST /webhooks/stripe` on payment-gateway.\n2. payment-gateway verifies signature (Stripe API version pinned to 2023-10-16).\n3. Raw event is persisted to Postgres `stripe_events` (primary key `event_id`).\n4. A background worker processes events and updates `payments`/`orders` state.\n5. checkout-service queries order/payment state from Postgres only (no Stripe polling).\n\nIdempotency:\n- Postgres unique constraint on `stripe_events.event_id`\n- Redis key: `stripe:evt:{event_id}` TTL 7 days (optimization, not correctness)\n\n# Quick triage (first 10 minutes)\n## 1) Check Stripe dashboard delivery status\nLook for the endpoint `.../webhooks/stripe`. Common patterns:\n- 400 `invalid_signature`\n- 413 payload too large (ingress/body size)\n- 429 (we are throttling)\n- 500 (we error before persisting)\n\nIf Stripe is showing retries with 5xx, treat as a backlog incident; Stripe will continue retrying for hours.\n\n## 2) Check payment-gateway logs\n```bash\nkubectl -n ecommerce-prod logs deploy/payment-gateway --since=15m | egrep 'StripeSignature|invalid_signature|raw body|413|payload|ETIMEDOUT|deadlock'\n```\nTypical messages:\n- `StripeSignatureVerificationError: No signatures found matching the expected signature for payload`\n- `request entity too large` (ingress)\n- `Error: connect ETIMEDOUT postgres` (DB)\n- `deadlock detected` (rare; DB)\n\n## 3) Validate raw body handling is intact\nWebhook verification requires the unmodified raw request body. If someone enabled `express.json()` globally before the webhook route, signature checks will fail.\nCode must look like:\n```js\napp.post('/webhooks/stripe',\n  express.raw({ type: 'application/json' }),\n  handleStripeWebhook\n)\n```\nIf not, rollback immediately.\n\n## 4) Check Postgres write health\nIf payment-gateway can’t persist `stripe_events`, we must return 500 so Stripe retries.\nRun:\n```sql\nselect count(*) from stripe_events where created_at > now() - interval '10 minutes';\nselect max(created_at) from stripe_events;\n```\nIf `max(created_at)` is stale, persistence is broken.\n\n# Mitigation playbook\nChoose based on observed error type.\n\n## Mitigation A: Signature failures (400)\n**Goal:** stop rejecting valid Stripe events.\n1. Confirm `STRIPE_WEBHOOK_SECRET` matches Stripe endpoint secret.\n```bash\nkubectl -n ecommerce-prod exec deploy/payment-gateway -- printenv | grep STRIPE_WEBHOOK_SECRET\n```\n2. Ensure ingress does not modify body:\n- NGINX ingress must have `proxy-request-buffering: \"on\"` and should not decompress/transform.\n- If a WAF started normalizing JSON, disable it for `/webhooks/stripe` path.\n3. Roll back payment-gateway to last known good tag (commonly `payment-gateway:1.38.2`).\n```bash\nkubectl -n ecommerce-prod rollout undo deploy/payment-gateway\n```\n\n## Mitigation B: Payload too large (413)\nThis can happen when events include expanded objects (e.g., invoice lines).\n1. Increase ingress body size for this path only.\nExample annotation in helm values:\n```yaml\nnginx.ingress.kubernetes.io/proxy-body-size: \"2m\"\n```\n2. Verify the path routing doesn’t go through a gateway with 1MB limit.\n\n## Mitigation C: We are slow / queue depth grows\nIf 2xx responses are happening but queue depth climbs, processing is slow.\n1. Increase worker concurrency cautiously:\n- `WEBHOOK_MAX_CONCURRENCY` from 20 -> 50\n```bash\nkubectl -n ecommerce-prod set env deploy/payment-gateway WEBHOOK_MAX_CONCURRENCY=50\n```\n2. Reduce DB contention:\n- Temporarily disable non-essential side effects (email receipts):\n  - `PAYMENTS_SEND_EMAILS=false`\n3. If DB is struggling, prefer “persist + defer processing”:\n- `WEBHOOK_DEFER_PROCESSING=true` (persist event, enqueue minimal job)\n4. If the queue is in Redis and Redis is failing, switch to in-memory queue (degraded):\n- `WEBHOOK_QUEUE_BACKEND=memory`\n**Caution:** memory queue is not durable; only use if Redis is the root cause and Stripe retries are working.\n\n## Mitigation D: Postgres timeouts / errors\nIf payment-gateway returns 500 due to DB timeouts, the backlog grows but Stripe will retry. The goal is to restore DB health first.\n1. Confirm this is not connection exhaustion (see RB-2024-11).\n2. Reduce webhook concurrency temporarily to reduce DB pressure:\n- `WEBHOOK_MAX_CONCURRENCY=10`\n3. If there are deadlocks, check recent schema changes and indexes on `stripe_events` and `payments`.\n\n# Safe replay / catch-up\nNormally, Stripe retries are enough. Manual replay is only needed if:\n- we returned 2xx without persisting (bug), or\n- we persisted but processing worker was down and Stripe stopped retrying (rare), or\n- support needs immediate correction for a VIP order.\n\nReplayer job:\n```bash\nkubectl -n ecommerce-prod create job --from=cronjob/stripe-webhook-replay stripe-webhook-replay-manual\n```\nRules:\n- Replayer must be idempotent (by `event_id`).\n- Replayer processes in created_at order but must tolerate out-of-order event types.\n\n# Verification\n- Stripe dashboard: recent attempts are 2xx.\n- `stripe_webhook_queue_depth` trends down.\n- Orders created in last 15 minutes should not accumulate in `PAYMENT_PENDING`.\n\nSQL spot checks:\n```sql\nselect payment_status, count(*)\nfrom payments\nwhere created_at > now() - interval '30 minutes'\ngroup by 1;\n\nselect type, count(*)\nfrom stripe_events\nwhere created_at > now() - interval '30 minutes'\ngroup by 1\norder by 2 desc;\n```\n\n# Edge cases\n- **Out-of-order delivery:** `charge.refunded` can arrive before `payment_intent.succeeded` during retries. Our processor must check current payment state and apply transitions safely.\n- **Duplicate events:** Stripe may deliver the same `evt_*` multiple times. Never delete the unique constraint to “fix” errors.\n- **Partial deploys:** if only some pods have updated webhook secret, you’ll see intermittent signature failures and a confusing mix of 2xx/400.\n- **Clock drift:** signature validation rejects if timestamp tolerance is exceeded. If only one node fails, check NTP; do not widen tolerance beyond 5 minutes without security review.\n\n# References\n- ADR-2024-12 Stripe webhooks as source of truth\n- Onboarding: Payments flow (OB-2025-04)\n- Postmortem: PM-2025-08 webhook retries and duplicate fulfillment"
    },
    {
      "doc_id": "rb-2025-12-es-cluster-red",
      "title": "Runbook: Elasticsearch Cluster Red / Search Degradation",
      "document_type": "runbook",
      "created_at": "2025-12-03",
      "content": "# Summary\nUse this runbook when product search is degraded due to Elasticsearch cluster health issues (yellow/red), shard allocation failures, threadpool rejections, or indexing backlogs. The primary symptom in production is search-api returning `503 SEARCH_UNAVAILABLE` or timing out while Elasticsearch reports `status: red` and `unassigned_shards > 0`.\n\n**Primary services:** search-api, indexing-worker  \n**Dependency:** Elasticsearch 8.11.3 (K8s, 3 master-eligible + 6 data nodes)  \n**Indexing source:** Postgres `catalog` schema via Kafka topic `catalog.product.changed`\n\n# What “normal” looks like\n- Cluster health: green\n- search-api p95: 120–220ms for `/api/search/products`\n- Threadpool:\n  - `search` queue near 0\n  - `write` queue near 0\n- Indexing lag (Kafka): < 50k messages, catches up within minutes after deploys\n- Storage: data nodes < 70% disk usage (below low watermark)\n\n# Symptoms\n## User-facing\n- Empty results for common queries (“sneakers” returns 0)\n- Filters missing (brand facet empty)\n- Search page loads forever then errors\n\n## Service errors\nsearch-api:\n- `elasticsearch.exceptions.ConnectionTimeout: Connection timed out`\n- `TransportError(429, 'es_rejected_execution_exception', ...)`\n- `index_not_found_exception: no such index [products_read]`\n\nindexing-worker:\n- `BulkIndexError: 200 document(s) failed to index`\n- `mapper_parsing_exception: failed to parse field [attributes.size]`\n\n# Dashboards and alerts\nGrafana:\n- **Search / search-api** (latency, 5xx, 503 rate)\n- **Platform / Elasticsearch / Cluster** (health, disk, JVM, threadpools)\n- **Indexing / Kafka Lag**\n\nPrometheus alerts (typical):\n- `ElasticsearchClusterRed` (cluster status red for 5m)\n- `ElasticsearchDiskHighWatermark` (disk usage > 90%)\n- `SearchApi503Spike` (rate of 503 > threshold)\n\n# Quick triage (first 10 minutes)\n## 1) Check cluster health\n```bash\nkubectl -n ecommerce-prod port-forward svc/elasticsearch 9200:9200\ncurl -s http://localhost:9200/_cluster/health?pretty\n```\nKey fields:\n- `status`: yellow/red is bad\n- `unassigned_shards`\n- `active_primary_shards`\n\n## 2) Check allocation explain\n```bash\ncurl -s -X GET http://localhost:9200/_cluster/allocation/explain?pretty\n```\nLook for causes:\n- `disk_threshold` (most common)\n- `NODE_LEFT` / `node_shutdown`\n- `shards_limit` / `total_shards_per_node`\n\n## 3) Confirm aliases exist\nWe query via alias `products_read` (see ADR-2025-02).\n```bash\ncurl -s http://localhost:9200/_cat/aliases/products_read?v\n```\nIf missing, search-api will fail even if cluster is green.\n\n## 4) Check threadpool rejections\n```bash\ncurl -s http://localhost:9200/_cat/thread_pool/search?v\ncurl -s http://localhost:9200/_cat/thread_pool/write?v\n```\nIf `rejected` increases, reduce load from search-api and/or indexing-worker.\n\n# Mitigation playbook\n## Mitigation A: Disk watermark / unassigned shards\nIf allocation explain shows `disk_threshold`:\n1. Identify old indices and sizes:\n```bash\ncurl -s 'http://localhost:9200/_cat/indices/products-v3-*?h=index,store.size,docs.count&s=index'\n```\n2. Delete indices beyond retention. Default retention is 14 days.\n```bash\ncurl -s -X DELETE 'http://localhost:9200/products-v3-2025.0*'\n```\n3. If deletion is blocked due to ILM, delete manually anyway during incident; fix ILM after.\n\n**Edge case:** If you delete the currently pointed index for `products_read`, search will go hard down. Always check alias target before deleting:\n```bash\ncurl -s 'http://localhost:9200/_cat/aliases/products_read?h=index'\n```\n\n## Mitigation B: Threadpool rejections (429)\nIf ES is overloaded but healthy:\n1. Put search-api into degraded mode:\n- `SEARCH_ENABLE_AGGS=false`\n- `SEARCH_ENABLE_SPELLCHECK=false`\n- `SEARCH_MAX_CONCURRENCY=40` (default 80)\n```bash\nkubectl -n ecommerce-prod set env deploy/search-api SEARCH_ENABLE_AGGS=false SEARCH_ENABLE_SPELLCHECK=false SEARCH_MAX_CONCURRENCY=40\n```\n2. If indexing-worker is also running, reduce pressure by pausing or reducing bulk size (see Mitigation C).\n\n## Mitigation C: Indexing runaway / bulk failures\nIf Kafka lag is huge or bulk indexing is failing:\n1. Pause indexing-worker to let ES recover:\n```bash\nkubectl -n ecommerce-prod scale deploy/indexing-worker --replicas=0\n```\n2. If the issue is mapping/field explosion, pausing buys time but doesn’t fix. Check the failure:\n- `Limit of total fields [1000] has been exceeded`\n- `mapper_parsing_exception` due to inconsistent types (string vs number)\n3. Reduce bulk size on resume:\n- `INDEXING_BULK_SIZE=250` (default 1000)\n- `INDEXING_FLUSH_INTERVAL_MS=750` (default 250)\nThese settings reduce merge pressure.\n\n## Mitigation D: Alias/mapping mismatch after deploy\nIf search-api returns `failed to find field`:\n- It’s usually querying a field removed/renamed in a new index mapping.\nActions:\n1. Roll back search-api to the last compatible version.\n2. If a new index was rolled out, repoint `products_read` alias to previous index as a rollback:\n```bash\ncurl -X POST localhost:9200/_aliases -H 'Content-Type: application/json' -d '{\n  \"actions\": [\n    {\"remove\": {\"alias\": \"products_read\", \"index\": \"products-v3-2025.02.06\"}},\n    {\"add\": {\"alias\": \"products_read\", \"index\": \"products-v3-2025.01.30\"}}\n  ]\n}'\n```\n\n# Recovery\n1. Cluster health green for 10+ minutes.\n2. search-api p95 back < 250ms.\n3. Indexing lag decreasing after workers resume.\n\nWhen resuming indexing-worker:\n- Start with 1 replica, bulk size 250, then scale to normal.\n\n# Edge cases and pitfalls\n- **Hot shards:** a single shard can be overloaded (popular category). Symptoms: normal health but high latency. Use:\n  ```bash\n  curl -s 'http://localhost:9200/_cat/shards/products-v3-*?v&s=store'\n  ```\n- **Master instability:** `master_not_discovered_exception` means cluster coordination problems. Don’t do manual shard allocation until masters stabilize.\n- **Segment merging storms:** if indexing bulk is too aggressive, CPU spikes and searches slow down. Degraded mode helps users while ES catches up.\n- **Type conflicts:** attribute fields (e.g., `attributes.size`) must be normalized in indexing-worker; otherwise mapping rejects.\n\n# References\n- ADR-2025-02 Index aliasing strategy for products\n- API Spec: search-api v3\n- Weekly ops checklist (verify retention and ILM health)"
    },
    {
      "doc_id": "rb-2025-06-k8s-rollout-stuck",
      "title": "Runbook: Kubernetes Rollout Stuck / CrashLoopBackOff",
      "document_type": "runbook",
      "created_at": "2025-06-21",
      "content": "# Summary\nThis runbook covers Kubernetes rollouts that get stuck, pods that CrashLoopBackOff, and deployments that silently degrade during release. It is written for our core services: checkout-service, cart-service, search-api (FastAPI) and payment-gateway (Node).\n\nCommon user-visible symptoms include sudden 5xx spikes right after deploys, readiness failures causing zero capacity, and partial rollouts where mixed versions produce inconsistent behavior.\n\n# Scope and prerequisites\nNamespace: `ecommerce-prod` (prod), `ecommerce-staging` (staging)  \nCluster: Kubernetes 1.29  \nIngress: nginx-ingress  \nStandard container ports:\n- FastAPI: 8080\n- Node payment-gateway: 8090\n\nHealth endpoints (expected):\n- `/health/live` (local-only checks)\n- `/health/ready` (may include dependency checks; see notes below)\n\n# Symptoms\n- `kubectl rollout status` hangs > 5 minutes\n- Pods show:\n  - `CrashLoopBackOff`\n  - `ImagePullBackOff`\n  - `OOMKilled`\n  - `Readiness probe failed`\n- User-facing:\n  - Increased 502/503 from ingress\n  - checkout-service `/api/checkout/start` returns 500 with validation errors\n\n# Quick triage\n## 1) Identify whether the rollout is blocked by readiness\n```bash\nkubectl -n ecommerce-prod rollout status deploy/checkout-service\nkubectl -n ecommerce-prod get rs -l app=checkout-service\nkubectl -n ecommerce-prod get pods -l app=checkout-service -o wide\n```\nIf new ReplicaSet has pods but they never become ready: it’s usually readiness probe or startup crash.\n\n## 2) Describe one failing pod\n```bash\nkubectl -n ecommerce-prod describe pod <pod>\n```\nLook at:\n- Events (image pull errors, probe failures)\n- Last termination reason (OOMKilled)\n- Environment (missing config)\n\n## 3) Check logs (current and previous)\n```bash\nkubectl -n ecommerce-prod logs <pod> --previous --tail=200\nkubectl -n ecommerce-prod logs <pod> --tail=200\n```\nCommon patterns:\n- Pydantic settings validation:\n  - `ValidationError: Settings.STRIPE_API_KEY Field required`\n- DB mismatch:\n  - `psycopg.errors.UndefinedColumn: column ... does not exist`\n- Redis DNS:\n  - `Name or service not known: redis.ecommerce-prod.svc`\n- Node webhook failures:\n  - `StripeSignatureVerificationError` (see Stripe runbook)\n\n# Mitigation paths\n## A) Readiness probe failures\n### Confirm readiness endpoint works locally\n```bash\nkubectl -n ecommerce-prod exec -it <pod> -- sh\nwget -qO- http://127.0.0.1:8080/health/ready\n```\nIf it fails:\n- If it fails due to dependency checks (Redis/ES), decide whether readiness should be strict.\n- We support `READINESS_STRICT_DEPS=false` for services where we prefer serving partial functionality.\n\n**Rule of thumb:** liveness must never call external systems. Readiness may, but be careful: strict readiness can cause total outage if Redis is flaky.\n\n### Temporary mitigation\n- For cart-service: set `READINESS_STRICT_DEPS=false` if Redis is down and you prefer DB fallback.\n```bash\nkubectl -n ecommerce-prod set env deploy/cart-service READINESS_STRICT_DEPS=false\n```\n\n## B) CrashLoopBackOff due to missing env var / secret\n1. Confirm the secret exists:\n```bash\nkubectl -n ecommerce-prod get secret checkout-service-secrets -o yaml | head\n```\n2. Compare the env var names in the Deployment vs the service settings docs.\n3. If a secret key was renamed, fastest safe option is rollback.\n\nRollback:\n```bash\nkubectl -n ecommerce-prod rollout undo deploy/checkout-service\n```\n\n## C) Schema migration mismatch\nIf logs show `UndefinedColumn` or `Can't locate revision`:\n1. Roll back immediately to restore capacity (unless you know migration is already running).\n2. Run migration job manually against primary Postgres:\n```bash\nkubectl -n ecommerce-prod create job --from=cronjob/checkout-migrations checkout-migrations-manual\n```\n3. Verify `alembic head` is applied before redeploy.\n\n**Important:** migrations must not go through PgBouncer (transaction pooling can break some migration patterns).\n\n## D) OOMKilled\n1. Confirm via describe: `Last State: Terminated (OOMKilled)`.\n2. Check Grafana: **K8s / Pod Memory** for the new version.\n3. Mitigation options:\n- Raise memory limit temporarily (e.g., 800Mi -> 1200Mi) and redeploy.\n- Roll back if memory growth is due to a regression (common: logging payloads, loading huge config, caching too much in-process).\n\n## E) ImagePullBackOff\n1. Confirm image tag exists in registry and the cluster has pull access.\n2. Common causes:\n- GitHub Actions pushed to the wrong repository\n- Promotion workflow referenced a tag that was never built\n- imagePullSecret missing in namespace\n\nFastest fix: repoint to last known good image, then fix CI.\n\n# Partial rollout hazards (mixed versions)\nEven when readiness works, partial rollouts can break invariants:\n- cart-service new key format `cart2:` vs old `cart:` leads to cache misses and higher DB load.\n- checkout-service expecting new error codes from cart-service results in 500 on unrecognized responses.\n\nIf you suspect mixed-version incompatibility:\n- Prefer rollback rather than “wait it out”.\n- If you must proceed, temporarily pin traffic (or scale down old ReplicaSet) only after verifying the new version is fully healthy.\n\n# Verification\nAfter mitigation:\n- Rollout completes.\n- Ingress 5xx returns to baseline.\n- Service error budgets stop burning.\n\nSmoke test commands:\n```bash\ncurl -sSf https://api.company.tld/healthz\ncurl -sSf https://api.company.tld/api/search/products?q=sneakers | head\n```\n\n# Post-incident follow-ups\n- Add a pre-deploy gate: verify required secrets and alembic head.\n- Ensure readiness endpoints are consistent and documented.\n- Link incident to PM-2025-07 migrations outage if relevant."
    },
    {
      "doc_id": "rb-2024-11-postgres-conn-exhaustion",
      "title": "Runbook: Postgres Connection Exhaustion",
      "document_type": "runbook",
      "created_at": "2024-11-09",
      "content": "# Summary\nThis runbook is for incidents where Postgres rejects new connections (`FATAL: sorry, too many clients already`) or becomes unstable due to too many concurrent sessions. This typically affects multiple services at once (checkout-service, cart-service, payment-gateway) because they share the same primary Postgres cluster.\n\nWe use PgBouncer in transaction pooling mode (ADR-2024-08). Most incidents come from services accidentally bypassing PgBouncer or increasing pool sizes in a deploy.\n\n# Symptoms\n## User-facing\n- Widespread 500s across cart/checkout\n- Requests hang then fail (timeouts) as pools block waiting for a connection\n\n## Logs\nFastAPI (psycopg/SQLAlchemy):\n- `psycopg.OperationalError: connection failed: FATAL: sorry, too many clients already`\n- `sqlalchemy.exc.TimeoutError: QueuePool limit of size 30 overflow 20 reached`\n\nNode payment-gateway (pg module):\n- `Error: remaining connection slots are reserved for non-replication superuser connections`\n\n## Metrics\n- Postgres `numbackends` near `max_connections` (prod default 600)\n- `pgbouncer_clients_waiting` increases (if PgBouncer is working but saturated)\n- Elevated latency on DB-dependent endpoints\n\n# Architecture notes\n- Primary Postgres endpoint (do not use directly from apps):\n  - `postgres-primary.ecommerce-prod.svc:5432`\n- PgBouncer endpoint (apps should use):\n  - `pgbouncer.ecommerce-prod.svc:6432`\n- Migrations (alembic) should connect to primary directly (not PgBouncer).\n\n# Quick triage\n## 1) Confirm connection counts and top offenders\nConnect as admin (read-only is fine for triage):\n```sql\nshow max_connections;\nselect count(*) as total from pg_stat_activity;\nselect application_name, usename, state, count(*)\nfrom pg_stat_activity\ngroup by 1,2,3\norder by count(*) desc\nlimit 15;\n```\nConventions (application_name):\n- `checkout-service@<pod>`\n- `cart-service@<pod>`\n- `payment-gateway@<pod>`\nIf application_name is blank or looks like a driver default, it may indicate a misconfigured connection string.\n\n## 2) Check for “idle in transaction”\n```sql\nselect pid, application_name, now() - xact_start as xact_age, query\nfrom pg_stat_activity\nwhere state = 'idle in transaction'\norder by xact_age desc\nlimit 20;\n```\nIf there are many long-lived idle-in-tx sessions, they can pin resources and block vacuum.\n\n## 3) Check PgBouncer health\nIf you can connect to PgBouncer admin:\n```sql\nshow pools;\nshow stats;\n```\nLook at:\n- `cl_active`, `cl_waiting`\n- server pool sizes\n\n# Mitigation options\n## Mitigation A: Reduce app concurrency (fastest and safest)\nFor FastAPI services, worker count multiplies DB pools.\n- Temporarily set `WEB_CONCURRENCY=2` (default 4–6 in prod)\n- Lower pool sizes:\n  - `DB_POOL_SIZE=10`\n  - `DB_MAX_OVERFLOW=5`\n```bash\nkubectl -n ecommerce-prod set env deploy/checkout-service WEB_CONCURRENCY=2 DB_POOL_SIZE=10 DB_MAX_OVERFLOW=5\nkubectl -n ecommerce-prod set env deploy/cart-service WEB_CONCURRENCY=2 DB_POOL_SIZE=10 DB_MAX_OVERFLOW=5\n```\nWatch Postgres connections after 2–3 minutes.\n\n## Mitigation B: Force apps back through PgBouncer\nIf a deploy changed `DATABASE_URL` to point at primary:\n1. Roll back or patch env var to PgBouncer:\n- `DATABASE_URL=postgresql+psycopg://app:***@pgbouncer.ecommerce-prod.svc:6432/ecommerce`\n2. Restart pods.\n\nThis is the most common “why did it suddenly happen?” root cause.\n\n## Mitigation C: Kill problematic sessions (last resort)\nOnly do this if you’ve identified a set of sessions that are clearly stuck and not doing useful work. Example: 1000 idle-in-tx sessions from one pod.\n```sql\nselect pg_terminate_backend(pid)\nfrom pg_stat_activity\nwhere application_name like 'checkout-service@%' and state='idle in transaction'\n  and now() - xact_start > interval '2 minutes';\n```\n**Warning:** terminating sessions can cause request failures and partial writes. Prefer to reduce load first.\n\n## Mitigation D: Enable rate limits to protect the DB\nIf the DB is close to collapsing (timeouts everywhere):\n- rate limit `/api/checkout/start` and `/api/cart/*` at ingress\n- prefer 429 over 500 cascades\n- disable non-essential workers (indexing-worker) temporarily\n\n# Recovery\nA good recovery state:\n- total connections < 420 (70% of max)\n- no growing backlog of requests waiting for DB connections\n- PgBouncer `cl_waiting` near 0\n\nAfter stabilization:\n1. Gradually restore `WEB_CONCURRENCY` and pool sizes.\n2. If you changed env vars, create a follow-up PR to fix defaults.\n3. Add a postmortem if incident > 15 minutes or user impact significant.\n\n# Edge cases and nuances\n- **Connection leaks:** missing `session.close()` on exception paths; manifests as slowly increasing connections after deploy.\n- **Async pool misuse:** mixing sync and async engines can double pools.\n- **Prepared statements:** PgBouncer transaction pooling breaks session-scoped prepared statements. If a service silently enables prepared statements, it may behave poorly under pooling.\n- **Background jobs:** refund-reconciler and indexing-worker can create surprising connection load; include them in offender analysis.\n- **Read replicas:** if read traffic is accidentally routed to primary due to DNS misconfig, you’ll see more connections without a deploy.\n\n# References\n- ADR-2024-08 PgBouncer transaction pooling\n- Runbook RB-2025-06 Kubernetes rollout stuck (often related)"
    },
    {
      "doc_id": "adr-2024-12-stripe-webhooks",
      "title": "ADR-2024-12: Stripe Webhooks as Source of Truth for Payment State",
      "document_type": "adr",
      "created_at": "2024-12-15",
      "content": "# ADR-2024-12: Stripe Webhooks as Source of Truth for Payment State\n\n## Status\nAccepted (2024-12-15)\n\n## Context\nOur payments integration uses Stripe for card payments and refunds. Historically, checkout-service performed **active polling** against Stripe (PaymentIntent retrieval) to determine final state. Polling created operational and product issues:\n\n- Cost and throttling: during peak traffic, we hit Stripe rate limits (`429 Too Many Requests`) and occasionally received `StripeRateLimitError`.\n- Race conditions: polling windows caused state oscillation (order shows pending even though payment succeeded).\n- Complex retries: checkout-service needed retry logic and backoff, increasing p95 latency and error rates.\n- Poor auditability: support had no canonical record of external payment events, only “last polled result”.\n\nWe already operate a legacy Node service (**payment-gateway**) that receives Stripe webhooks, but it was treated as “best effort” and the rest of the platform did not trust its output. As a result we had two competing sources of payment truth.\n\nWe want a single consistent mechanism, with strong idempotency and clear failure modes.\n\n## Decision\nStripe webhooks will be the **authoritative** source for payment state transitions.\n\n- payment-gateway will:\n  1) validate webhook signatures,\n  2) persist raw events to Postgres,\n  3) enqueue internal processing jobs.\n\n- checkout-service will:\n  - update `payments.payment_status` and `orders.status` only based on processed webhook events.\n  - stop polling Stripe in the checkout request path.\n\nPolling remains only as a scheduled reconciliation tool (daily) and for manual support tooling, not for user-facing state changes.\n\n## Technical design\n### Stripe API version pinning\nTo reduce unexpected event shape changes:\n- Pin Stripe API version in payment-gateway to `2023-10-16`.\n- Store the version used in logs for debugging (`stripe_api_version=2023-10-16`).\n\n### Webhook endpoint\n- Route: `POST /webhooks/stripe`\n- Ingress must preserve request body exactly (no JSON reformatting).\n- payment-gateway must use raw body parsing for signature verification:\n  - `express.raw({ type: 'application/json' })` for this route only.\n\n### Signature verification\n- Env var: `STRIPE_WEBHOOK_SECRET`\n- Reject with 400 if signature invalid.\nExpected error signature in logs:\n- `StripeSignatureVerificationError: No signatures found matching the expected signature for payload`\n\n### Persistence (audit log)\nPersist the entire event payload (JSON) in Postgres to support support/debugging and replay.\n\nSchema (approved):\n```sql\ncreate table stripe_events (\n  event_id text primary key,\n  type text not null,\n  created_at timestamptz not null default now(),\n  payload jsonb not null,\n  processed_at timestamptz,\n  process_error text\n);\ncreate index stripe_events_type_created_at_idx on stripe_events (type, created_at desc);\n```\n\n### Processing model\n- payment-gateway enqueues `process_stripe_event(event_id)` into its worker queue.\n- Worker loads event, maps it to internal transitions, writes updates in a single DB transaction.\n- Idempotency is guaranteed by `stripe_events.event_id` primary key and unique constraints on `payments.stripe_event_id`.\n\n### Event ordering\nStripe does not guarantee ordering. The processor must handle:\n- duplicates (same `event_id` replayed)\n- out-of-order (refund-related events arriving before success events in retries)\n\nImplementation rule:\n- always read current payment record before applying transition;\n- transitions are monotonic where possible (e.g., don’t go from `PAID` back to `PENDING`).\n\n### Failure behavior\nIf payment-gateway cannot persist the event:\n- return 500 to Stripe so Stripe retries.\n- do **not** return 2xx without persistence; that creates silent data loss.\n\n## Consequences\n### Positive\n- Reduced Stripe API usage and fewer 429s.\n- Clear audit trail of all incoming external payment events.\n- Cleaner separation: checkout-service orchestrates, payment-gateway integrates with Stripe.\n\n### Negative / risks\n- Operational coupling: webhook ingestion becomes critical path for payment finalization.\n- If webhook signature verification breaks (e.g., body parsing regression), payment state stalls and orders remain pending.\n- Requires runbooks and alerting around queue depth and webhook success rate (see RB-2026-01).\n\n## Alternatives considered\n1. **Continue polling** in checkout-service: rejected due to cost, complexity, and latency.\n2. **Move webhook ingestion directly into checkout-service**: rejected because Stripe keys and webhook secrets are currently owned by payment-gateway, and checkout-service is not yet ready for key custody.\n3. **Publish Stripe events to Kafka**: deferred. Could help decouple processing, but adds infra and operational complexity. Revisit after payment-gateway migration to Python.\n\n## Notes / messy details\n- We must keep `proxy-body-size` high enough for occasional large Stripe events (invoices). Past incidents showed 413s when default ingress limit was 1m.\n- If Redis is down, idempotency still works via Postgres uniqueness, but processing may be slower. Don’t treat Redis as correctness layer here."
    },
    {
      "doc_id": "adr-2025-03-redis-carts",
      "title": "ADR-2025-03: Redis as Primary Cache for Carts",
      "document_type": "adr",
      "created_at": "2025-03-11",
      "content": "# ADR-2025-03: Redis as Primary Cache for Carts (Read-Through + Write-Through)\n\n## Status\nAccepted (2025-03-11)\n\n## Context\ncart-service is a high-QPS service. During peak events (sales, campaigns), cart reads become one of the top 3 DB load drivers. Before this ADR, cart-service hit Postgres on most reads. Even with indexes, p95 latency drifted above 450ms under load, which then impacted checkout-service because it fetches cart snapshots during checkout.\n\nWe tried in-process caching inside the FastAPI pods, but it had problems:\n- cache warm-up varied by pod; horizontal scaling caused cold starts\n- deploys flush caches and create “cache storms”\n- per-pod memory growth was unpredictable and caused OOMKilled in staging\n\nRedis is already in our stack (sessions, rate limits, Stripe idempotency), and we can leverage it as a shared cache.\n\n## Decision\nUse Redis as the primary cache for cart payloads with Postgres as the source of truth.\n\n- cart-service will implement:\n  - read-through cache for GET cart\n  - write-through cache update on mutations\n  - TTL-based expiration with jitter\n- Redis will use an eviction policy optimized for “hot carts”:\n  - `maxmemory-policy allkeys-lfu`\n\nThe decision explicitly does **not** make Redis the system of record for carts.\n\n## Technical design\n### Key format\nKey format is stable and must not change without a compatibility plan:\n- `cart:{user_id}:v{version}`\n\nRationale:\n- user_id is enough to address cart in our product model\n- version supports optimistic concurrency and helps avoid lost updates\n\n### TTL\n- Base TTL: 900 seconds (`CART_CACHE_TTL_SECONDS=900`)\n- TTL jitter: 0–120 seconds (`CART_CACHE_TTL_JITTER_SECONDS=120`) to avoid synchronized expiry storms\n\n### Serialization format\n- msgpack v2 payload (smaller and faster than JSON)\n- cart payload includes:\n  - cart_id, version, currency\n  - items (sku, qty, unit_price_minor)\n  - minimal totals (subtotal, tax estimate)\n\nExplicitly excluded (for now):\n- full recommendation metadata\n- customer profile fields\n\n### Redis configuration\nBaseline config (prod):\n```conf\nmaxmemory 18gb\nmaxmemory-policy allkeys-lfu\nactivedefrag yes\nlazyfree-lazy-eviction yes\n```\n\n### cart-service behavior\nRead path (`GET /api/cart/{user_id}`):\n1. If `CART_CACHE_BYPASS=true`, go to Postgres.\n2. Otherwise:\n   - try Redis GET\n   - on hit: return, set `X-Cache: hit`\n   - on miss: load from Postgres, write to Redis, set `X-Cache: miss`\n\nWrite path (mutations):\n- Update Postgres in a transaction.\n- Increment cart `version`.\n- Write new payload to Redis.\n- Publish `cart.updated` event (used by downstream analytics, not required for correctness).\n\n### ETags / concurrency contract\ncheckout-service uses `If-Match` with cart ETag for optimistic concurrency. cart-service returns:\n- `ETag: W/<etag>`\n- where `<etag>` is derived from `(version, item_hashes)`.\n\nNote: if ETag derivation depends on cached payload and Redis evicts keys, we might see increased mismatch rates. See “Consequences” and PM-2025-11.\n\n### Failure mode\nIf Redis is unavailable or error rate is high:\n- cart-service falls back to Postgres\n- it should not fail requests just because cache is down\n- it emits `redis_errors_total` and sets `X-Cache: bypass`\n\n## Consequences\n### Positive\n- Lower Postgres read load during peaks.\n- Improved latency and better checkout stability under normal conditions.\n\n### Negative / risks\n- Redis becomes a larger blast radius: eviction storms can push load back to Postgres suddenly.\n- Key/value size drift can cause memory pressure; we need monitoring for value size and eviction rate.\n- Partial deploy risk: changing serialization or key format can cause near-100% misses.\n\n## Alternatives considered\n1. **Postgres-only with more read replicas**: rejected; increases cost and still has higher p95 under spikes.\n2. **Store carts fully in Redis (source of truth)**: rejected; durability/consistency risk.\n3. **Client-side caching**: rejected; correctness issues and hard invalidation.\n\n## Follow-ups / requirements\n- Add alerting on `redis_evicted_keys_total` and `cart_cache_value_size_bytes`.\n- Add a hard cap on cart cache payload size (proposal: 128KB) and skip caching if exceeded.\n- Document incident response (see RB-2026-02 Redis evictions).\n- Add compatibility test ensuring key format remains stable.\n\n## Notes\nThis ADR optimizes for performance. If we observe repeated eviction-driven incidents, we may need to move toward a smaller cached representation or a different cache strategy (e.g., caching only item ids and prices, not computed totals)."
    },
    {
      "doc_id": "adr-2025-02-es-aliasing",
      "title": "ADR-2025-02: Elasticsearch Aliasing and Zero-Downtime Reindexing",
      "document_type": "adr",
      "created_at": "2025-02-06",
      "content": "# ADR-2025-02: Elasticsearch Index Aliasing and Zero-Downtime Reindexing for Products\n\n## Status\nAccepted (2025-02-06)\n\n## Context\nProduct search uses Elasticsearch. We frequently need to evolve mappings (new analyzers, new fields, changed types). Elasticsearch does not allow many mapping changes in-place. Reindexing the full catalog (tens of millions of documents) takes hours, and doing it “live” risks downtime.\n\nPreviously, we used a single rolling index (e.g., `products-current`) and updated mappings whenever possible. This led to:\n- risky deploys when a mapping change was required\n- limited rollback options (mapping changes are hard to undo)\n- partial failures where search-api queries and mappings diverged\n\nWe want a predictable operational pattern that supports:\n- safe reindexing\n- atomic cutover\n- fast rollback\n- compatibility during deploy windows\n\n## Decision\nAdopt a read/write alias strategy:\n- Read alias: `products_read`\n- Write alias: `products_write`\n- Concrete indices: `products-v3-YYYY.MM.DD` (date is index creation date, not data date)\n\nindexing-worker writes to `products_write`. search-api reads from `products_read`. During reindex windows, these aliases may point to different concrete indices.\n\n## Technical design\n### Index template and mapping controls\nTemplate name: `products-template-v3`\nKey rules:\n- Dynamic mapping is restricted for attribute-like fields:\n  - `attributes.*` must be normalized to known types to avoid field explosion.\n- Set `index.mapping.total_fields.limit` to 1000 to prevent pathological documents.\n\nExample (partial) template settings:\n```json\n{\n  \"index_patterns\": [\"products-v3-*\"],\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 12,\n      \"number_of_replicas\": 1,\n      \"index.mapping.total_fields.limit\": 1000\n    }\n  }\n}\n```\n\n### Cutover process (standard)\n1. Create new index `products-v3-<today>` with updated template.\n2. Backfill:\n   - preferred: reindex from Postgres snapshot (catalog export)\n   - fallback: Elasticsearch `_reindex` from previous index\n3. Enable dual-write in indexing-worker for a fixed window:\n   - `INDEX_DUAL_WRITE=true`\n   - `INDEX_DUAL_WRITE_TARGET=products-v3-<today>`\n4. Switch `products_read` alias atomically to new index.\n5. After validation (query parity + doc counts), switch `products_write` to new index and disable dual-write.\n\nAlias switch example:\n```bash\ncurl -X POST localhost:9200/_aliases -H 'Content-Type: application/json' -d '{\n  \"actions\": [\n    {\"remove\": {\"alias\": \"products_read\", \"index\": \"products-v3-2025.01.30\"}},\n    {\"add\":    {\"alias\": \"products_read\", \"index\": \"products-v3-2025.02.06\"}}\n  ]\n}'\n```\n\n### Validation criteria\n- `/_cat/indices` shows doc count within 0.5% of expected catalog size.\n- Search parity checks:\n  - 20 canonical queries have comparable totals (+/- 5%)\n  - filters return non-empty facets for common brands\n- No sustained `query_shard_exception` in search-api logs.\n\n### Rollback\nRollback is repointing `products_read` back to prior index. This is the key reason for aliases.\n\n## Consequences\n### Positive\n- Zero-downtime mapping evolution\n- Fast rollback without needing to restore an old cluster state\n- Safer deploy windows: search-api can be upgraded independently\n\n### Negative / risks\n- Storage overhead: during dual-write, two indices exist simultaneously.\n- Operational complexity: alias switch and ILM/retention patterns must be maintained.\n- New failure mode: if `products_read` alias is missing or points to a deleted index, search-api errors hard (`index_not_found_exception`).\n\n## Alternatives considered\n1. **Single index with ILM only**: rejected; ILM manages lifecycle, not mapping evolution.\n2. **Use OpenSearch or a managed search service**: deferred; not in scope and would require broader migration.\n3. **Blue/green full cluster**: rejected due to cost and complexity.\n\n## Edge cases\n- Disk watermark: if data nodes cross watermark, shards become unassigned and alias switching won’t help (see ES runbook).\n- Mapping conflicts: if a field changes type (string -> number), documents will fail to index; dual-write must be monitored for `mapper_parsing_exception`.\n- Analyzer changes: query relevance may change; product team must sign off on relevance diffs.\n\n## Follow-ups\n- Ensure ILM policies reference `products-v3-*` patterns after each alias strategy change.\n- Add weekly checklist items for retention and ILM health.\n\n## Operational conventions\n- Index naming is **creation date**, not “data date”. Do not create indices named by ingestion date; it breaks retention checks.\n- ILM policy (current): `products-ilm-v3`\n  - hot phase: rollover at 50GB\n  - delete phase: 14 days\n- Retention is enforced by ILM, but during incidents oncall may delete indices manually; follow up must restore ILM correctness."
    },
    {
      "doc_id": "adr-2024-08-pgbouncer",
      "title": "ADR-2024-08: PgBouncer Transaction Pooling for Postgres",
      "document_type": "adr",
      "created_at": "2024-08-22",
      "content": "# ADR-2024-08: Introduce PgBouncer Transaction Pooling for Postgres\n\n## Status\nAccepted (2024-08-22)\n\n## Context\nWe run multiple services (checkout-service, cart-service, search-api, payment-gateway) and background workers. Each component previously maintained its own connection pool directly to the Postgres primary. Under load, the total number of open connections became the sum of:\n- web pods * (workers per pod) * (pool size)\n- plus workers (indexing, refunds, webhook processing)\n\nThis regularly exceeded Postgres capacity and caused incidents:\n- `FATAL: sorry, too many clients already`\n- cascading timeouts as pools blocked\n- elevated p95 latency due to connection churn\n\nScaling Postgres `max_connections` is not a good primary fix because it increases memory usage per backend and can degrade performance.\n\n## Decision\nIntroduce PgBouncer as a connection pooler in **transaction pooling** mode for application traffic.\n\n- All application services must connect via PgBouncer (`:6432`) by default.\n- Direct connections to Postgres primary (`:5432`) are reserved for:\n  - migrations (alembic)\n  - admin/debugging sessions\n  - explicitly approved exceptions\n\n## Technical design\n### PgBouncer mode\nWe choose `pool_mode = transaction` because:\n- it maximizes reuse of server connections\n- it handles high concurrency better than session pooling\n\n### Core settings (prod baseline)\n```ini\npool_mode = transaction\nmax_client_conn = 5000\ndefault_pool_size = 50\nreserve_pool_size = 10\nserver_idle_timeout = 60\nquery_wait_timeout = 120\nignore_startup_parameters = extra_float_digits\n```\n\n### Service configuration\nFastAPI (SQLAlchemy 2.0 + psycopg):\n- Use `DATABASE_URL` pointing at PgBouncer:\n  - `postgresql+psycopg://app:***@pgbouncer.ecommerce-prod.svc:6432/ecommerce`\n- Keep client-side pool small to avoid “pool-on-pool” issues:\n  - `DB_POOL_SIZE=10`\n  - `DB_MAX_OVERFLOW=5`\n\nNode payment-gateway (`pg`):\n- `max: 20` in pool config (client-side)\n- Ensure `statement_timeout` and `query_timeout` are set per transaction (cannot rely on session state)\n\n### Compatibility constraints\nTransaction pooling breaks session-level assumptions:\n- prepared statements across transactions\n- temp tables that persist across requests\n- session settings (`SET ROLE`, `SET search_path`) must be applied each transaction\n\nPolicy:\n- If a service requires session features, it must document why and request an exception. Exception traffic must be rate-limited.\n\n### Monitoring\nWe will monitor:\n- PgBouncer `cl_waiting` and `sv_active`\n- Postgres `numbackends`\n- Error rate: `timeout waiting for server connection`\n\n## Consequences\n### Positive\n- Dramatically reduces Postgres backend count.\n- More stable p95 latency under spikes.\n- Clear operational boundary: PgBouncer absorbs connection churn.\n\n### Negative / risks\n- Adds a critical component; PgBouncer outage affects all services.\n- Some libraries “quietly” rely on session features; bugs can appear after migration.\n- Migrations must bypass PgBouncer; otherwise failures can be confusing.\n\n## Rollout plan\n1. Deploy PgBouncer into staging and validate with one service (cart-service).\n2. Migrate services one-by-one, keeping the ability to revert quickly.\n3. Add a deploy checklist item: ensure `DATABASE_URL` points to PgBouncer.\n4. Add runbook and alerting for connection exhaustion incidents (see RB-2024-11).\n\n## Edge cases and notes\n- If PgBouncer is saturated, clients will queue and latency will rise even if Postgres is healthy. Watch `query_wait_timeout`.\n- If a deploy accidentally points a service at Postgres primary, connection count spikes quickly; treat as SEV-2 if it threatens `max_connections`.\n- Alembic migrations run as Kubernetes jobs; these jobs must use primary Postgres and must not share the app’s PgBouncer URL.\n\n## Alternatives considered\n1. **Increase Postgres max_connections**: rejected; higher memory footprint and still vulnerable to bursts.\n2. **Service-by-service pool tuning** only: rejected; error-prone and easy to regress in deploys.\n3. **Use a managed Postgres proxy**: deferred; would reduce ops but adds vendor coupling.\n\n## Validation\nSuccess criteria after rollout:\n- Postgres `numbackends` stays < 300 during peak.\n- No sustained `too many clients` errors.\n- PgBouncer `cl_waiting` remains near 0 under normal load.\n\nOperational drill:\n- Temporarily scale checkout-service web pods up by 2x in staging and confirm Postgres connections remain stable.\n- Simulate PgBouncer restart and verify services recover without manual intervention (connection retry logic must exist).\n\n## Related documents\n- RB-2024-11 Postgres connection exhaustion\n- CL-2025-09 Production deploy checklist (verify DB endpoints)"
    },
    {
      "doc_id": "ob-2025-05-fastapi-local",
      "title": "Onboarding: Working on FastAPI Services Locally",
      "document_type": "onboarding",
      "created_at": "2025-05-05",
      "content": "# Onboarding: Working on FastAPI Services Locally (checkout-service, cart-service, search-api)\n\n## Scope\nThis guide gets you from “fresh laptop” to running our core FastAPI services with realistic dependencies. It’s opinionated and includes the things that actually trip people up (ports, env vars, migrations, seed data).\n\nServices covered:\n- checkout-service (FastAPI 0.110, Python 3.11, SQLAlchemy 2.0, psycopg)\n- cart-service (FastAPI 0.110, Python 3.11, Redis cache)\n- search-api (FastAPI 0.110, Python 3.11, Elasticsearch client 8.x)\n\nLegacy service (not covered in depth here):\n- payment-gateway (Node 18) – see Payments onboarding.\n\n## Prerequisites\n- Docker Desktop 4.27+ (or Linux Docker Engine)\n- Python 3.11.7\n- Poetry 1.8+\n- Make (optional, but our scripts assume it exists)\n- For k8s dev: kubectl 1.29+, kind 0.22+\n\n## Repo layout\n- `checkout-service/`\n- `cart-service/`\n- `search-api/`\n- `infra/` (docker-compose, local k8s manifests)\n- `docs/` (conventions, runbooks, ADRs)\n\n## Start dependencies via docker-compose\nFrom `infra/`:\n```bash\ndocker compose up -d postgres redis elasticsearch\ndocker compose ps\n```\nDefault endpoints:\n- Postgres: `localhost:5432` db `ecommerce`, user `app`, password `app`\n- Redis: `localhost:6379` db 0\n- Elasticsearch: `http://localhost:9200` (single-node, no auth)\n\nIf docker-compose fails due to port conflicts, the fastest fix is to edit `infra/docker-compose.yml` ports and update your env vars accordingly.\n\n## Database bootstrap\nWe use Alembic migrations. There is no “shared migrations repo”; each service owns its own schema changes. For local dev you typically run checkout-service migrations first because it creates core order/payment tables.\n\n```bash\ncd checkout-service\npoetry install\nexport DATABASE_URL=postgresql+psycopg://app:app@localhost:5432/ecommerce\npoetry run alembic upgrade head\n```\n\nCommon migration errors:\n- `Can't locate revision identified by '...'` → your repo is out of date or your local DB has a weird migration history. If in doubt, drop and recreate local DB.\n- `psycopg.errors.InsufficientPrivilege` → you’re not using the `app` user from compose.\n\n## Running checkout-service\n```bash\ncd checkout-service\nexport DATABASE_URL=postgresql+psycopg://app:app@localhost:5432/ecommerce\nexport REDIS_URL=redis://localhost:6379/0\nexport PAYMENT_GATEWAY_URL=http://localhost:8090\nexport SERVICE_ENV=local\npoetry run uvicorn app.main:app --reload --port 8081\n```\n\nHealth endpoints:\n- `GET /health/live` (no dependency checks)\n- `GET /health/ready` (depends on DB; Redis optional)\n\nIf startup fails with:\n- `ValidationError: Settings.STRIPE_API_KEY Field required`\nSet a dummy value locally:\n```bash\nexport STRIPE_API_KEY=sk_test_dummy\n```\nWe do not hit Stripe directly from checkout-service, but the settings model is shared.\n\n## Running cart-service\n```bash\ncd cart-service\npoetry install\nexport DATABASE_URL=postgresql+psycopg://app:app@localhost:5432/ecommerce\nexport REDIS_URL=redis://localhost:6379/0\nexport CART_CACHE_TTL_SECONDS=900\nexport READINESS_STRICT_DEPS=false\npoetry run uvicorn app.main:app --reload --port 8082\n```\nNote: `READINESS_STRICT_DEPS=false` is recommended locally because Redis restarts are common and you still want the service to run using Postgres fallback.\n\n## Running search-api\n```bash\ncd search-api\npoetry install\nexport ELASTICSEARCH_URL=http://localhost:9200\nexport SERVICE_ENV=local\npoetry run uvicorn app.main:app --reload --port 8083\n```\nInitialize local index:\n```bash\npython scripts/create_index.py --index products-v3-local\npython scripts/seed_products.py --count 500\n```\nIf you see:\n- `index_not_found_exception: no such index [products_read]`\nThat’s because production uses aliases. Locally, either create the alias or configure `SEARCH_INDEX_NAME=products-v3-local`.\n\n## Tests\n- Unit tests:\n```bash\npoetry run pytest -q\n```\n- Integration tests require docker-compose deps:\n```bash\npytest -q -m integration\n```\nIf tests hang, it’s usually waiting for Postgres. Check `DATABASE_URL` and that compose is running.\n\n## Debugging conventions\n- We propagate `X-Request-Id` across services. You can set it manually in curl.\n- Standard error body:\n```json\n{ \"error_code\": \"...\", \"message\": \"...\", \"request_id\": \"...\" }\n```\n- For cart cache debugging (staging only), you can enable:\n  - `LOG_CACHE_KEYS=true` to see key formats (do not do in prod).\n\n## “Gotchas” that waste time\n- Poetry env mismatch: if `poetry install` pulls Python 3.12, you’ll hit dependency pins. Force Python 3.11.\n- Elasticsearch mapping drift: if you change templates, delete and recreate local index.\n- Mixed ports: we use 8081/8082/8083 locally; prod uses 8080 inside containers. Don’t copy-paste Kubernetes probes into local scripts without adjusting.\n\n## Next documents to read\n- API specs: cart-service v1, checkout-service v2, search-api v3\n- Runbooks: Redis evictions, Stripe webhook backlog, K8s rollout stuck"
    },
    {
      "doc_id": "ob-2025-04-payments",
      "title": "Onboarding: Payments and Stripe Integration",
      "document_type": "onboarding",
      "created_at": "2025-04-02",
      "content": "# Onboarding: Payments and Stripe Integration (checkout-service + payment-gateway)\n\n## Why this doc exists\nPayments span two services and two “worlds”:\n- user-facing orchestration (checkout-service)\n- Stripe integration + webhook ingestion (payment-gateway)\n\nMost bugs happen at the seams: idempotency, retries, and state transitions. This doc focuses on how things actually work in prod today.\n\n## Components\n- checkout-service (FastAPI): creates orders, requests payment intent creation, reads payment state from Postgres\n- payment-gateway (Node 18): owns Stripe keys, creates PaymentIntents, ingests webhooks\n- Postgres: durable state (`orders`, `payments`, `stripe_events`)\n- Redis: optimization for idempotency (`stripe:evt:*`) and some internal locks\n- Stripe: external payment provider\n\n## High-level flow (happy path)\n1. Client calls checkout-service:\n   - `POST /api/checkout/start` with `Idempotency-Key`\n2. checkout-service validates cart snapshot by calling cart-service with `If-Match` ETag.\n3. checkout-service persists:\n   - `orders` row with status `PAYMENT_PENDING`\n   - `payments` row with `provider='stripe'`, `payment_status='PENDING'`\n4. checkout-service calls payment-gateway internal API:\n   - `POST /internal/stripe/payment_intents`\n5. payment-gateway creates PaymentIntent in Stripe (API version 2023-10-16) and returns `client_secret`.\n6. Client completes card confirmation in the browser.\n7. Stripe sends webhook(s) to payment-gateway.\n8. payment-gateway persists `stripe_events` and processes them.\n9. checkout-service observes updated payment status and transitions order status to `PAID`.\n\n## State machine (simplified)\npayments.payment_status:\n- `PENDING` -> `SUCCEEDED` -> `REFUNDED`\n- failures: `FAILED`, `CANCELED`\n\norders.status:\n- `PAYMENT_PENDING` -> `PAID` -> `FULFILLING` -> `FULFILLED`\n- failures: `PAYMENT_FAILED`, `CANCELLED`\n\nRule: checkout-service is the only service that updates `orders.status`, but it does so based on `payments.payment_status` changes.\n\n## Important invariants\n### Idempotency is mandatory\n- External: checkout-service requires `Idempotency-Key` for `/api/checkout/start`.\n- Internal: payment-gateway maps (order_id, idempotency_key) to a single Stripe PaymentIntent.\n\nIf a client retries due to a network timeout, it must provide the same `Idempotency-Key`, otherwise you can create duplicate PaymentIntents. This is a real support issue.\n\n### Stripe webhooks are authoritative\nWe do not poll Stripe to decide if a payment succeeded (ADR-2024-12). If webhook processing is delayed, order status stays pending. That’s expected; the fix is to restore webhook processing (see Stripe runbook).\n\n### Raw body is required for signature verification\nIf the webhook route parses JSON before verifying signatures, verification fails. This is why we treat `/webhooks/stripe` specially and do not use global `express.json()` there.\n\n## Local dev setup\n### Running payment-gateway locally\n```bash\ncd payment-gateway\nnpm ci\nexport STRIPE_API_KEY=sk_test_...\nexport STRIPE_WEBHOOK_SECRET=whsec_...\nexport DATABASE_URL=postgres://app:app@localhost:5432/ecommerce\nexport REDIS_URL=redis://localhost:6379/0\nnpm run dev\n```\nDefault port: 8090\n\n### Stripe CLI for local webhook forwarding\n```bash\nstripe listen --forward-to localhost:8090/webhooks/stripe\n```\nThe Stripe CLI prints a `whsec_...` secret; set that as `STRIPE_WEBHOOK_SECRET`.\n\nCommon error:\n- `invalid_signature` → you forwarded to the wrong path or your secret is wrong.\n\n## Debugging common scenarios\n### “Stripe shows succeeded but our order is pending”\nCheck `stripe_events` freshness:\n```sql\nselect max(created_at) from stripe_events;\nselect * from payments where order_id = '<id>';\n```\nIf events are arriving but not processed, check `processed_at` and `process_error`.\n\n### “Duplicate fulfillment happened”\nThis usually means:\n- we processed duplicate events incorrectly, or\n- our internal fulfillment consumer is not idempotent.\n\nFirst, verify event idempotency:\n```sql\nselect event_id, count(*) from stripe_events group by 1 having count(*) > 1;\n```\nYou should never see duplicates because event_id is primary key. If you do, schema is broken.\n\n### “Refund requested, never completed”\nRefunds can be partial and multi-step:\n- request creates a refund object\n- Stripe later sends `charge.refunded` or refund updated events\n\nEnsure refund-reconciler job runs daily and can catch up if webhook was missed.\n\n## Edge cases you should know\n- 3DS flows: PaymentIntent can be `requires_action` for minutes; do not treat as failure.\n- Amount rounding: all amounts in Stripe are minor units; ensure `unit_price_minor` is used consistently.\n- Currency: we currently support `USD` and `EUR` only. Stripe may accept others in test mode; prod should reject.\n- Webhook ordering: refund-related events can arrive “before” success events in retries. Processor must be monotonic.\n\n## References\n- ADR-2024-12 Stripe webhooks source of truth\n- Runbook: Stripe webhook backlog\n- API spec: checkout-service v2"
    },
    {
      "doc_id": "ob-2025-10-cicd-k8s",
      "title": "Onboarding: CI/CD and Kubernetes Conventions",
      "document_type": "onboarding",
      "created_at": "2025-10-12",
      "content": "# Onboarding: CI/CD and Kubernetes in This Platform (GitHub Actions + Docker + K8s)\n\n## Overview\nThis doc explains how code moves from a PR to production and how to debug common CI and deploy issues. It’s not a Kubernetes tutorial; it’s the conventions and sharp edges specific to our stack.\n\n## Environments and namespaces\n- Staging: `ecommerce-staging`\n- Production: `ecommerce-prod`\n\nIngress hostnames:\n- staging: `staging-api.company.tld`\n- prod: `api.company.tld`\n\n## Container images\nWe build and push images per service.\nNaming convention:\n- `<service>:<git-sha>` (immutable build artifact)\n- `<service>:v<semver>` (release tag, points to a sha)\n\nExample:\n- `checkout-service:9f2c1a3`\n- `checkout-service:v2.14.0`\n\nDo not deploy “latest”.\n\n## GitHub Actions workflows\nEach service has:\n- `.github/workflows/ci.yml` (runs on PR)\n- `.github/workflows/release.yml` (tags + builds release image)\nPlatform has:\n- `.github/workflows/promote.yml` (promotes to prod)\n\nTypical CI jobs:\n1. `lint`:\n   - ruff 0.4.x\n   - mypy (select services)\n2. `test`:\n   - pytest (unit + integration depending on service)\n3. `build`:\n   - docker buildx\n4. `push`:\n   - push to registry\n\nSnippet (simplified):\n```yaml\n- name: Build\n  run: docker buildx build -t registry/company/checkout-service:${{ github.sha }} .\n- name: Push\n  run: docker push registry/company/checkout-service:${{ github.sha }}\n```\n\nCommon CI failure:\n- “tests pass locally but fail in CI” → missing service dependency (Redis/ES) or different env var defaults.\n\n## Deploy process (happy path)\n1. Merge to `main` triggers auto-deploy to staging.\n2. Validate staging (smoke tests).\n3. Trigger `promote.yml` with the image SHA to deploy to production.\n4. Watch rollout status and dashboards for 15 minutes.\n\n## Kubernetes resources and conventions\n- Deployments: web services (checkout-service, cart-service, search-api, payment-gateway)\n- CronJobs: migrations, refund-reconciler, Stripe replay\n- ConfigMaps: non-secret config (timeouts, feature flags)\n- Secrets: Stripe keys, DB passwords\n\nPorts:\n- containerPort 8080 (FastAPI)\n- containerPort 8090 (payment-gateway)\n\nHealth probes:\n- liveness: `/health/live`\n- readiness: `/health/ready`\n\n**Important:** liveness must not depend on Postgres/Redis/ES. If it does, transient dependency failures turn into restart storms.\n\n## Debugging deploy failures\n### Rollout stuck\nUse the runbook RB-2025-06. Fast path:\n```bash\nkubectl -n ecommerce-prod rollout status deploy/cart-service\nkubectl -n ecommerce-prod get pods -l app=cart-service\nkubectl -n ecommerce-prod describe pod <pod>\nkubectl -n ecommerce-prod logs <pod> --previous\n```\n\n### Config mistakes\nFastAPI services validate env vars at startup (pydantic). Missing secrets fail fast with a `ValidationError`. If you see this in prod, rollback is usually faster than patching unless you know the exact missing key.\n\n### Migrations and schema drift\nIf a deploy adds a DB column, you must ensure migrations run (PM-2025-07 describes what happens otherwise). Our promote workflow is supposed to run migration jobs, but verify it.\n\n## Observability basics\n- Services export Prometheus metrics at `/metrics`.\n- Grafana dashboards are organized by service and platform components.\n- Always include `X-Request-Id` when debugging cross-service calls.\n\n## Edge cases\n- Partial rollouts can create mixed versions and inconsistent cache formats (Redis key changes). Prefer rollback.\n- If your deploy changes Redis usage, watch `redis_evicted_keys_total` and cart mismatch errors for at least 30 minutes.\n- If your deploy changes search mappings, validate `products_read` alias and retention settings.\n\n## Related docs\n- Checklist: Production deploy\n- Runbooks: Rollout stuck, Postgres connection exhaustion, Redis evictions\n\n## Local Kubernetes (optional but useful)\nWe use kind for local k8s smoke tests (mostly for ingress/probes).\n```bash\nkind create cluster --name ecommerce-dev\nkubectl create ns ecommerce-dev\nkubectl -n ecommerce-dev apply -f infra/k8s/dev/\n```\nIf pods fail due to image pull, load images into kind:\n```bash\nkind load docker-image checkout-service:local --name ecommerce-dev\n```\n\n## Rollback guidance\nRollback is normal. If a production deploy triggers a sudden error spike, roll back first, debug second.\n```bash\nkubectl -n ecommerce-prod rollout undo deploy/checkout-service\nkubectl -n ecommerce-prod rollout undo deploy/cart-service\n```\nAfter rollback, capture logs and metrics for the postmortem.\n\n## Secret rotation note\nStripe secrets are rotated quarterly. If you change `STRIPE_WEBHOOK_SECRET`, you must roll all payment-gateway pods quickly; partial rollout causes intermittent signature failures."
    },
    {
      "doc_id": "api-2025-11-cart-v1",
      "title": "API Spec: cart-service Public API (v1)",
      "document_type": "api_spec",
      "created_at": "2025-11-10",
      "content": "# API Spec: cart-service Public API (v1)\n\n## Overview\ncart-service owns cart state and exposes endpoints to read and mutate a user’s cart. Postgres is the system of record. Redis is a performance cache (ADR-2025-03) and must not affect correctness: if cache is down, endpoints should still work (with higher latency).\n\nBase path: `/api/cart`  \nService: `cart-service` (FastAPI 0.110)  \nAuth: Bearer token required for all public endpoints  \nIdempotency: Required for mutation endpoints via `Idempotency-Key`\n\n## Headers\n### Request\n- `Authorization: Bearer <jwt>`\n- `Idempotency-Key: <uuid>` (mutations)\n- `If-Match: W/<etag>` (mutations; optimistic concurrency)\n- `X-Request-Id: <string>` (optional; if absent, generated)\n\n### Response\n- `ETag: W/<etag>` (GET and successful mutations)\n- `X-Cache: hit|miss|bypass` (informational)\n- `X-Request-Id: <string>`\n\n## Data model\n### Cart\n- `cart_id` (uuid)\n- `user_id` (uuid)\n- `version` (int, increments on each mutation)\n- `currency` (string; currently `USD` or `EUR`)\n- `items` (array of CartItem)\n- `totals` (object; includes `subtotal_minor`, `tax_minor`, `total_minor`)\n- `updated_at` (RFC3339)\n\n### CartItem\n- `sku` (string, required)\n- `qty` (int 1..99)\n- `unit_price_minor` (int, >= 0)\n- `title` (string, optional; may be omitted in some responses)\n\nNote: price is captured at time of cart update; final checkout may reprice (checkout-service responsibility).\n\n## Error format\nAll errors use:\n```json\n{ \"error_code\": \"SOME_CODE\", \"message\": \"Human readable\", \"request_id\": \"...\" }\n```\n\nCommon error codes:\n- `CART_NOT_FOUND` (404)\n- `ITEM_NOT_FOUND` (404)\n- `INVALID_QTY` (422)\n- `INVALID_SKU` (422)\n- `CART_VERSION_MISMATCH` (409)\n- `PRECONDITION_FAILED` (412; missing or invalid If-Match)\n- `RATE_LIMITED` (429)\n- `UPSTREAM_PRICING_UNAVAILABLE` (503; if pricing dependency is enabled)\n\n## Endpoints\n### GET /api/cart/{user_id}\nReturns the current cart. If none exists, returns an empty cart object (not 404) unless `include_empty=false`.\n\nQuery params:\n- `include_empty` (bool, default true)\n\nResponses:\n- 200: Cart payload\n- 304: Not modified (when `If-None-Match` used)\n\nExample request:\n```bash\ncurl -H \"Authorization: Bearer $TOKEN\" \\\n     -H \"If-None-Match: W/\\\\\\\"c17:abc\\\\\\\"\" \\\n     https://api.company.tld/api/cart/9a9a...\n```\n\nExample response 200:\n```json\n{\n  \"cart_id\": \"3b3b6f9e-0f2e-4f0c-9d8a-1b2a3c4d5e6f\",\n  \"user_id\": \"9a9a...\",\n  \"version\": 17,\n  \"currency\": \"USD\",\n  \"items\": [{\"sku\": \"SKU-123\", \"qty\": 2, \"unit_price_minor\": 1299}],\n  \"totals\": {\"subtotal_minor\": 2598, \"tax_minor\": 182, \"total_minor\": 2780},\n  \"updated_at\": \"2025-11-02T10:12:30Z\"\n}\n```\n\n### POST /api/cart/{user_id}/items\nAdds or updates an item quantity.\n\nHeaders:\n- `Idempotency-Key` required\n- `If-Match` required (use latest ETag from GET)\n\nRequest body:\n```json\n{ \"sku\": \"SKU-123\", \"qty\": 3 }\n```\n\nResponses:\n- 200: updated cart\n- 409: `CART_VERSION_MISMATCH` (ETag does not match current cart version)\n- 412: `PRECONDITION_FAILED` (missing If-Match)\n- 422: `INVALID_QTY` or `INVALID_SKU`\n\nExample mismatch response:\n```json\n{ \"error_code\": \"CART_VERSION_MISMATCH\", \"message\": \"Cart changed, refetch required\", \"request_id\": \"...\" }\n```\n\n### DELETE /api/cart/{user_id}/items/{sku}\nRemoves an item.\n\nHeaders:\n- `Idempotency-Key` required\n- `If-Match` required\n\nResponses:\n- 200: updated cart\n- 404: `ITEM_NOT_FOUND`\n\n### POST /api/cart/{user_id}/clear\nClears the cart entirely (sets items to empty, increments version).\n\nThis is used by UI “clear cart” and by checkout-service after successful order placement (optional; feature-flagged).\n\n### GET /api/cart/{user_id}/items\nReturns items in a paginated way for extremely large carts (rare, but support has seen > 500 items due to API abuse).\n\nQuery params:\n- `page_size` (int, default 100, max 200)\n- `cursor` (opaque string)\n\nResponse includes:\n- `next_cursor` (null if end)\n\n## Idempotency behavior\ncart-service stores idempotency keys in Redis:\n- `idem:cart:{user_id}:{key}` TTL 24 hours\nIf Redis is down, we fall back to Postgres table `idempotency_keys` (slower).\n\nIf the same idempotency key is replayed with a different payload, return:\n- 409 `IDEMPOTENCY_KEY_REUSE`\n\n## Edge cases and notes\n- **Race with checkout:** checkout-service may read cart and then mutate it (e.g., applying coupon) while the user also changes qty. Clients must handle 409 and refetch.\n- **Qty 0:** rejected; use DELETE to remove items. (We treat qty=0 as ambiguous because some clients tried to use it as a remove operation, creating silent mismatches.)\n- **SKU reprice:** if pricing changes, cart-service does not automatically update unit_price_minor; checkout-service performs final pricing validation and may reject checkout with `PRICE_CHANGED`.\n- **Redis evictions:** can increase mismatch rate if ETag derivation relies on cached payload. See RB-2026-02 and PM-2025-11.\n\n## Changelog notes\nv1.6 (2025-10): added `/clear` endpoint  \nv1.7 (2025-11): added `IDEMPOTENCY_KEY_REUSE` error"
    },
    {
      "doc_id": "api-2025-11-checkout-v2",
      "title": "API Spec: checkout-service Public API (v2)",
      "document_type": "api_spec",
      "created_at": "2025-11-18",
      "content": "# API Spec: checkout-service Public API (v2)\n\n## Overview\ncheckout-service orchestrates checkout, order creation, and payment initiation via payment-gateway. It reads cart state from cart-service and persists orders/payments in Postgres. Payment completion is driven by Stripe webhooks (ADR-2024-12).\n\nBase path: `/api/checkout`  \nService: `checkout-service` (FastAPI 0.110)  \nAuth: Bearer token required for all endpoints\n\n## Headers\n### Request\n- `Authorization: Bearer <jwt>`\n- `Idempotency-Key: <uuid>` (required for `/start` and refund endpoints)\n- `X-Request-Id` (optional)\n\n### Response\n- `X-Request-Id`\n- `Retry-After` (optional; when returning 503 due to provider outage)\n\n## Error format\n```json\n{ \"error_code\": \"CODE\", \"message\": \"Text\", \"request_id\": \"...\" }\n```\nCommon error codes:\n- `CART_VERSION_MISMATCH` (409)\n- `PRICE_CHANGED` (409)\n- `ADDRESS_INVALID` (422)\n- `PAYMENT_PROVIDER_UNAVAILABLE` (503)\n- `PAYMENT_PENDING_TIMEOUT` (409; pending too long)\n- `ORDER_NOT_FOUND` (404)\n- `ORDER_ALREADY_PAID` (409)\n\n## State model\norders.status:\n- `DRAFT` (rare; internal)\n- `PAYMENT_PENDING`\n- `PAID`\n- `FULFILLING`\n- `FULFILLED`\n- terminal failures: `PAYMENT_FAILED`, `CANCELLED`\n\npayments.payment_status:\n- `PENDING`, `SUCCEEDED`, `FAILED`, `CANCELED`, `REFUNDED`\n\n## Endpoints\n### POST /api/checkout/start\nCreates an order and returns a Stripe client secret.\n\nHeaders:\n- `Idempotency-Key` required\n\nRequest body:\n```json\n{\n  \"user_id\": \"uuid\",\n  \"cart_id\": \"uuid\",\n  \"shipping_address_id\": \"uuid\",\n  \"billing_address_id\": \"uuid\",\n  \"currency\": \"USD\"\n}\n```\n\nProcessing notes:\n1. Calls cart-service `GET /api/cart/{user_id}` and validates ETag.\n2. Reprices items via pricing rules (internal module, not a service call).\n3. Persists order + payment rows in a single DB transaction.\n4. Calls payment-gateway internal endpoint to create PaymentIntent.\n\nSuccess response 201:\n```json\n{\n  \"order_id\": \"uuid\",\n  \"status\": \"PAYMENT_PENDING\",\n  \"payment\": {\n    \"provider\": \"stripe\",\n    \"payment_intent_id\": \"pi_...\",\n    \"client_secret\": \"pi_..._secret_...\"\n  }\n}\n```\n\nFailure responses:\n- 409 `CART_VERSION_MISMATCH`: cart changed after UI loaded; client must refetch cart and retry.\n- 409 `PRICE_CHANGED`: totals differ after repricing; client must refresh totals before retry.\n- 503 `PAYMENT_PROVIDER_UNAVAILABLE`: payment-gateway timed out or returned error. Response includes `retry_after_seconds`.\n\nEdge case: if payment-gateway times out but Stripe created the intent, idempotency ensures we return the same intent on retry. If idempotency is broken, you’ll see duplicate PaymentIntents in Stripe.\n\n### GET /api/checkout/orders/{order_id}\nReturns order details and current status.\n\nResponse 200 includes:\n- order totals\n- payment status\n- timestamps (created_at, updated_at)\n\n### GET /api/checkout/orders/{order_id}/status\nLightweight status endpoint for polling UI:\n```json\n{ \"order_id\": \"...\", \"status\": \"PAYMENT_PENDING\", \"payment_status\": \"PENDING\" }\n```\nThis endpoint is cached for 2 seconds at the edge; do not use for data correctness.\n\n### POST /api/checkout/orders/{order_id}/cancel\nCancels an unpaid order.\n\nRules:\n- If payment_status is `SUCCEEDED`, return 409 `ORDER_ALREADY_PAID`.\n- If payment is still pending and older than 10 minutes, cancel is allowed and marks order `CANCELLED`.\n\n### POST /api/checkout/orders/{order_id}/refund\nRequests a refund (partial or full). Used by support tooling (requires elevated scopes in JWT).\n\nHeaders:\n- `Idempotency-Key` required\n\nRequest:\n```json\n{ \"amount_minor\": 2780, \"reason\": \"customer_request\" }\n```\n\nResponse:\n- 202 accepted (refund is asynchronous)\n- refund completion is driven by Stripe events and refund-reconciler\n\n## Idempotency details\n- For `/start`, `Idempotency-Key` is stored in Postgres `checkout_idempotency` with unique constraint `(user_id, key)`.\n- Replaying the key returns the same `order_id` and `client_secret` if the original request succeeded.\n- If the same key is replayed with different cart_id or address_id, return 409 `IDEMPOTENCY_KEY_REUSE`.\n\n## Observability\nKey metrics:\n- `checkout_start_total{status}`\n- `checkout_cart_mismatch_total`\n- `payment_provider_errors_total`\n- `orders_payment_pending_total`\n\nLog fields:\n- `order_id`, `user_id`, `cart_id`, `stripe_payment_intent_id`, `request_id`\n\n## Edge cases and nuances\n- **Pending too long:** If `PAYMENT_PENDING` exceeds 10 minutes, UI can show a “Check payment” button; backend may return `PAYMENT_PENDING_TIMEOUT` to trigger support flow.\n- **Out-of-order Stripe events:** payment can succeed after we cancelled due to timeout. Our processor must prefer success and then trigger refund if needed; this is rare but real in retries.\n- **Currency mismatch:** We reject orders if cart currency differs from request currency (422 `CURRENCY_MISMATCH`), but some older clients omit currency; default is USD.\n- **Cart 304:** cart-service may return 304 Not Modified; checkout-service must still read latest persisted cart snapshot to avoid trusting stale UI totals.\n\n## Related docs\n- ADR-2024-12 Stripe webhooks\n- Runbook: Stripe webhook backlog\n- Onboarding: Payments flow"
    },
    {
      "doc_id": "api-2025-11-search-v3",
      "title": "API Spec: search-api Public API (v3)",
      "document_type": "api_spec",
      "created_at": "2025-11-22",
      "content": "# API Spec: search-api Public API (v3)\n\n## Overview\nsearch-api provides product search backed by Elasticsearch. The API is stable even as index mappings evolve via aliases (ADR-2025-02). Search results are not guaranteed to be identical across index versions during reindex windows, but the contract is stable.\n\nBase path: `/api/search`  \nService: `search-api` (FastAPI 0.110)  \nAuth:\n- optional for anonymous search\n- required for personalized boosts (`Authorization: Bearer <jwt>`)\n\n## Request headers\n- `X-Request-Id` (optional)\n- `Accept-Language` (optional; affects analyzer selection for some locales)\n\n## Response headers\n- `X-Request-Id`\n- `X-Search-Degraded: true|false` (when degraded mode enabled)\n\n## Error format\n```json\n{ \"error_code\": \"CODE\", \"message\": \"Text\", \"request_id\": \"...\" }\n```\nError codes:\n- `INVALID_FILTER` (400)\n- `INVALID_SORT` (400)\n- `SEARCH_UNAVAILABLE` (503)\n- `SEARCH_RATE_LIMITED` (429)\n\n## Endpoints\n### GET /api/search/products\nQuery params:\n- `q` (string; empty means “trending”)\n- `page` (int, default 1, min 1)\n- `page_size` (int, default 24, max 100)\n- `filters` (repeatable; format `key:value`)\n  - supported keys: `brand`, `category`, `price_min`, `price_max`, `in_stock`\n- `sort` (`relevance|price_asc|price_desc|newest`)\n- `include_facets` (bool, default true)\n\nResponse 200:\n```json\n{\n  \"total\": 1234,\n  \"page\": 1,\n  \"page_size\": 24,\n  \"results\": [\n    { \"sku\": \"SKU-123\", \"title\": \"Sneaker X\", \"price_minor\": 1299, \"currency\": \"USD\", \"score\": 12.3 }\n  ],\n  \"facets\": {\n    \"brand\": [{\"value\": \"acme\", \"count\": 120}],\n    \"category\": [{\"value\": \"shoes\", \"count\": 900}]\n  }\n}\n```\n\n### GET /api/search/suggest\nAutocomplete suggestions.\n\nQuery params:\n- `q` (string, required)\n- `limit` (int, default 8, max 20)\n\nResponse:\n```json\n{ \"suggestions\": [\"sneakers\", \"sneaker cleaner\", \"sneaker socks\"] }\n```\n\n## Timeouts and retries\nsearch-api sets Elasticsearch client timeouts:\n- connect: 50ms\n- request: 150ms\n\nRetries are disabled to avoid amplifying load during ES overload. If ES returns 429 `es_rejected_execution_exception`, search-api returns:\n- 503 `SEARCH_UNAVAILABLE` with `retry_after_seconds: 2`\n\n## Degraded mode\nWhen ES is overloaded or cluster health is yellow/red:\n- set `SEARCH_ENABLE_AGGS=false` to disable facets\n- set `SEARCH_ENABLE_SPELLCHECK=false`\n- reduce concurrency (`SEARCH_MAX_CONCURRENCY=40`)\n\nIn degraded mode:\n- responses omit `facets`\n- header `X-Search-Degraded: true` is set\n\n## Mapping and alias assumptions\nsearch-api reads from alias:\n- `products_read`\n\nIf alias is missing or points to deleted index:\n- ES returns `index_not_found_exception`\n- search-api returns 503 with `SEARCH_UNAVAILABLE`\n\n## Edge cases\n- Empty query (`q=`): returns trending products cached for 60 seconds. If cache is cold, it runs a lightweight ES query against a curated category list.\n- Filter parsing errors: unknown filter keys return 400 `INVALID_FILTER` with the unknown key in message.\n- Locale analyzers: if `Accept-Language` is unsupported, default analyzer is used; results may be less relevant but should not error.\n- Stale facets during reindex: during dual-write, some categories may have lower counts. This is acceptable; do not treat as bug unless totals are wildly off.\n\n## Observability\nMetrics:\n- `search_requests_total{status}`\n- `search_latency_seconds_bucket`\n- `es_rejections_total`\n- `search_degraded_mode_enabled` (gauge)\n\n## Related docs\n- ADR-2025-02 aliasing and reindex strategy\n- Runbook: ES cluster red / search degradation\n\n## Rate limiting\nAnonymous search is rate limited at the edge:\n- 60 requests / minute / IP for `/products`\n- 120 requests / minute / IP for `/suggest`\n\nWhen limited, response:\n- 429 `SEARCH_RATE_LIMITED`\n- header `Retry-After: 5`\n\nAuthenticated users have higher limits (handled by gateway), but search-api still enforces a soft cap via `SEARCH_MAX_CONCURRENCY` to protect Elasticsearch.\n\n## Internal query behavior (stable contract, variable implementation)\nWe translate request params into an internal query model:\n- match query against `title`, `brand`, `categories`, `keywords`\n- apply filters as bool must clauses\n- ranking:\n  - base BM25 score\n  - boost for `in_stock=true`\n  - optional personalization boost for logged-in users (recently viewed categories)\n\nWe do not expose ES query DSL publicly. However, for debugging in staging you can set header:\n- `X-Debug-Search: true`\nand search-api will include a truncated `debug.query_hash` in the response. It will not include full DSL.\n\n## Additional error scenarios\n- If Elasticsearch is reachable but returns malformed response (rare), search-api returns 502 `SEARCH_UPSTREAM_BAD_RESPONSE`.\n- If the ES client times out (`ConnectionTimeout`), search-api returns 503 with `retry_after_seconds: 1`.\n- If request params exceed limits (e.g., 200 filters), search-api returns 400 `REQUEST_TOO_LARGE`.\n\n## Examples\nBasic query:\n```bash\ncurl 'https://api.company.tld/api/search/products?q=sneakers&page=1&page_size=24&filters=brand:acme&sort=relevance'\n```\n\nSuggest query:\n```bash\ncurl 'https://api.company.tld/api/search/suggest?q=snea&limit=8'\n```\n\n## Changelog\nv3.2 (2025-09): added `newest` sort  \nv3.3 (2025-11): added `X-Search-Degraded` header"
    },
    {
      "doc_id": "pm-2025-11-redis-evictions",
      "title": "Postmortem PM-2025-11: Redis OOM/Evictions Broke Checkout Consistency",
      "document_type": "postmortem",
      "created_at": "2025-11-30",
      "content": "# Postmortem PM-2025-11: Redis OOM/Evictions Broke Checkout Consistency\n\n## Summary\nOn 2025-11-29, Redis in `ecommerce-prod` experienced sustained memory pressure leading to heavy evictions and intermittent command latency. Because cart-service depends on Redis for high-performance cart reads (ADR-2025-03) and checkout-service relies on cart ETags for optimistic concurrency, cache churn caused a spike in `409 CART_VERSION_MISMATCH` and `412 PRECONDITION_FAILED` during checkout.\n\nThis incident did not lose cart data (Postgres remained source of truth), but it materially reduced conversion for ~47 minutes.\n\n## Impact\n- Time window: 2025-11-29 09:12–09:59 UTC (47 minutes)\n- Checkout start failures:\n  - 12.4% of `/api/checkout/start` requests returned 409/412\n  - p95 checkout latency increased from 220ms to 780ms\n- Cart page:\n  - p95 increased from 120ms to 410ms\n  - elevated “missing items” reports (mostly transient refresh issues)\n\n## Detection\nAlerts fired:\n- `RedisEvictionsHigh` (`increase(redis_evicted_keys_total[5m]) > 500`)\n- `CartVersionMismatchSpike` (checkout-service 409 rate > 5/s)\n\nOncall confirmation:\n- Grafana **Platform / Redis / Cache Health** showed `used_memory` near `maxmemory` and `evicted_keys` rising sharply.\n- cart-service dashboard showed cache miss ratio > 0.9.\n\n## Timeline (UTC)\n- 09:12: `RedisEvictionsHigh` alert triggered.\n- 09:16: checkout-service `http_409_total` spikes; support reports “cart changed” loops.\n- 09:18: Oncall checks Redis INFO; sees `used_memory` ~17.8GB / 18GB, `evicted_keys` rising.\n- 09:22: Mitigation: enable `CART_CACHE_BYPASS=true` and `CART_CACHE_WRITEBACK=false` on cart-service.\n- 09:26: Postgres read load rises but remains below 75% CPU; checkout 409 rate begins to drop.\n- 09:31: Oncall deletes a subset of oversized cart keys (last resort) to reduce Redis memory pressure.\n- 09:38: Hotfix prepared: remove embedded recommendation metadata from cart cache payload.\n- 09:45: Gradual re-enable cache reads, then writes.\n- 09:59: Metrics return to baseline; incident closed.\n\n## Root cause\nA cart-service feature launched earlier that morning included “recommended products” on the cart page. The implementation stored expanded recommendation metadata inside the cached cart payload. This increased average `cart:*` value size by ~6x (from ~30KB to ~180KB; p95 exceeded 220KB for some users).\n\nRedis maxmemory policy was `allkeys-lfu`, which protected frequently accessed keys but could not prevent eviction churn once overall memory pressure exceeded the configured 18GB. The combination of:\n- larger values,\n- a high volume of cart reads,\n- synchronized TTL expirations (insufficient jitter on some keys),\ncaused thrash: keys were evicted and immediately re-created.\n\nBecause ETag derivation in cart-service relied on the cached payload hash for some code paths, simultaneous rehydration led to transient ETag mismatches and 409 storms.\n\n## Contributing factors\n- Missing guardrails: no hard cap on cache payload size; caching should have been skipped for large payloads.\n- Load test gap: pre-release tests used small carts and did not include recommendation metadata shape.\n- Inconsistent TTL jitter: one code path for “cart with recommendations” used a fixed TTL with no jitter.\n- checkout-service did not retry on cart mismatch; it treated 409 as a hard failure.\n\n## Resolution and recovery\nShort-term:\n- Bypassed Redis cache for cart reads, reducing mismatch but increasing DB load.\n- Deleted a subset of large cart keys to reduce memory pressure.\n- Deployed hotfix to store only recommendation SKUs (not full metadata) inside cached cart payload.\n\nRecovery validation:\n- `redis_evicted_keys_total` flat for 15 minutes\n- cart-service cache miss ratio fell to < 0.5 after re-enable\n- checkout-service 409 rate returned to baseline\n\n## What went well\n- Feature flag `CART_CACHE_BYPASS` existed and was documented.\n- PgBouncer prevented Postgres connection exhaustion when read load spiked.\n- Oncall had direct access to dashboards and could correlate symptoms quickly.\n\n## What went poorly\n- A performance optimization became a correctness-adjacent failure mode (checkout mismatch).\n- Manual key deletion increased DB load and briefly increased latency (trade-off accepted).\n- We did not have visibility into per-key-prefix value sizes until after the incident.\n\n## Action items\n1. Implement `CART_CACHE_MAX_BYTES=131072` (128KB) and skip caching beyond cap.\n2. Add metric: `cart_cache_value_size_bytes` (p50/p95) and alert on spikes.\n3. Ensure TTL jitter is applied uniformly; add unit tests around TTL setting logic.\n4. Update checkout-service to retry once on `CART_VERSION_MISMATCH` by refetching cart and refreshing ETag.\n5. Update ADR-2025-03 with an explicit “do not cache expanded metadata” guideline.\n\n## Related docs\n- ADR-2025-03 Redis as primary cache for carts\n- RB-2026-02 Redis evictions causing cart staleness"
    },
    {
      "doc_id": "pm-2025-07-schema-drift",
      "title": "Postmortem PM-2025-07: Checkout Deploy Caused DB Schema Drift",
      "document_type": "postmortem",
      "created_at": "2025-07-19",
      "content": "# Postmortem PM-2025-07: Checkout Deploy Caused DB Schema Drift and 500s\n\n## Summary\nOn 2025-07-18 we deployed checkout-service v2.9.0 to production. The service expected a new column (`orders.applied_promotions`) introduced by an Alembic migration, but the migration job failed silently in the promote workflow. As a result, new pods started returning 500s with `psycopg.errors.UndefinedColumn`.\n\nThis incident illustrates why “migrations are part of the deploy” and why the promote pipeline must fail closed when migrations do not run.\n\n## Impact\n- Time window: 2025-07-18 14:03–14:24 UTC (21 minutes)\n- Affected endpoints:\n  - `POST /api/checkout/start` returned 500 for ~68% of requests\n  - `GET /api/checkout/orders/{id}` returned 500 for orders created during the window\n- User impact:\n  - checkout conversion drop; many customers retried and eventually succeeded after rollback\n\n## Detection\n- Alert: `Checkout5xxSpike` fired at 14:05 UTC\n- Logs: repeated errors:\n  - `psycopg.errors.UndefinedColumn: column orders.applied_promotions does not exist`\n- Grafana: checkout-service error budget burn exceeded threshold\n\n## Timeline (UTC)\n- 14:03: promote workflow completed; new pods begin rollout.\n- 14:05: 5xx alert fired.\n- 14:07: Oncall inspects logs and identifies missing DB column.\n- 14:10: Rollback initiated to v2.8.3.\n- 14:14: Rollback complete; error rate drops.\n- 14:18: Migration job manually run (to prepare for later redeploy).\n- 14:24: Incident resolved; postmortem initiated.\n\n## Root cause\nThe promote workflow step intended to run Alembic migrations used the **application DATABASE_URL**, which points to PgBouncer. PgBouncer is in transaction pooling mode (ADR-2024-08). The migration job attempted to run a migration that used advisory locks and expected session-level behavior. Under PgBouncer transaction pooling, the lock acquisition was not stable and the job exited with a non-zero status.\n\nThe workflow did not mark the pipeline as failed because:\n- the migration step was marked `continue-on-error: true` to avoid “blocking releases for transient DB issues” (bad decision)\n- there was no explicit check that alembic head was applied\n\nSo the deploy proceeded with code expecting the new column, causing runtime errors.\n\n## Contributing factors\n- CI vs prod drift: migrations succeeded in staging because staging used direct Postgres endpoint for migrations.\n- Insufficient safeguards: no automated “schema compatibility check” between code and DB.\n- Lack of visibility: migration job logs were not surfaced in the promote summary; oncall had to dig in the cluster.\n\n## Resolution\n- Immediate rollback to restore service.\n- Manually ran migration job against Postgres primary (bypassing PgBouncer) using a Kubernetes Job:\n```bash\nkubectl -n ecommerce-prod create job --from=cronjob/checkout-migrations checkout-migrations-manual\nkubectl -n ecommerce-prod logs job/checkout-migrations-manual\n```\n- Confirmed schema applied:\n```sql\nselect column_name from information_schema.columns\nwhere table_name='orders' and column_name='applied_promotions';\n```\n\n## What went well\n- Rollback was fast and reliable.\n- Error was deterministic and easy to diagnose from logs.\n\n## What went poorly\n- The deploy pipeline hid migration failures.\n- We relied on PgBouncer URL for migrations, contradicting ADR-2024-08 guidance.\n- No canary or preflight test caught schema incompatibility.\n\n## Action items\n1. Remove `continue-on-error` from migration step in promote workflow.\n2. Add explicit gate: `alembic current == head` before deploying new pods.\n3. Ensure migrations use `DATABASE_URL_PRIMARY` (direct Postgres) in all environments.\n4. Add a lightweight “schema check” startup probe for checkout-service:\n   - on startup, query required columns and fail fast in staging (not prod) to catch drift.\n5. Update onboarding CI/CD doc to emphasize migration endpoint difference.\n\n## Related docs\n- ADR-2024-08 PgBouncer transaction pooling\n- RB-2025-06 Kubernetes rollout stuck (schema drift often looks like CrashLoopBackOff)\n\n## Lessons learned (nuanced)\n- Some migrations are “safe” under transaction pooling, but any migration that depends on session state (temp tables, session variables, advisory locks held across statements) is not. The cost of debugging intermittent migration failures is higher than the cost of running migrations against the primary directly.\n- “Fail open” pipelines feel convenient until they ship an incompatible binary. For schema-bound services like checkout, a blocked deploy is preferable to a live outage.\n\nWe will treat migrations as a first-class release artifact and include their logs in the promote summary so oncall can see failures immediately."
    },
    {
      "doc_id": "pm-2025-08-duplicate-fulfillment",
      "title": "Postmortem PM-2025-08: Stripe Webhook Retries Triggered Duplicate Fulfillment",
      "document_type": "postmortem",
      "created_at": "2025-08-10",
      "content": "# Postmortem PM-2025-08: Stripe Webhook Retries Triggered Duplicate Fulfillment\n\n## Summary\nOn 2025-08-09, a brief period of payment-gateway instability caused Stripe to retry webhook deliveries. Our webhook processor handled the duplicate Stripe events correctly (idempotent by `event_id`), but a downstream internal consumer (`fulfillment-dispatcher`) was not idempotent on our own event stream. As a result, ~183 orders were dispatched to fulfillment twice.\n\nCustomers were not double-charged, but warehouse operations created duplicate shipment labels for some orders. We caught the issue quickly and canceled duplicates before most left the warehouse.\n\n## Impact\n- Time window: 2025-08-09 16:22–16:41 UTC (19 minutes)\n- Affected orders: 183 with duplicate dispatch events\n- Downstream effects:\n  - 61 duplicate shipping labels created\n  - 9 packages physically prepared twice (stopped before pickup)\n- Customer impact:\n  - No double charges\n  - Some customers saw confusing “two shipments” notifications\n\n## Detection\n- Alert: `FulfillmentDuplicateDispatchSpike` (custom alert in **Fulfillment / Dispatcher** folder)\n- Support tickets: “Two tracking numbers for one order”\n- payment-gateway alert: elevated 5xx at `/webhooks/stripe` and increasing `stripe_webhook_queue_depth`\n\n## Timeline (UTC)\n- 16:22: payment-gateway pods begin restarting due to OOMKilled (root cause below).\n- 16:24: Stripe observes 5xx and retries `payment_intent.succeeded` events.\n- 16:26: payment-gateway recovers; webhook ingestion resumes.\n- 16:28: checkout-service sees payments succeed and emits internal `order.paid` events.\n- 16:31: fulfillment-dispatcher dispatches orders; duplicates start appearing.\n- 16:34: Alert fires; oncall investigates and pauses fulfillment-dispatcher.\n- 16:37: Dedupe script run to cancel duplicate dispatches and labels.\n- 16:41: Incident stabilized; dispatcher resumed with temporary guard.\n\n## Root cause\nTwo issues combined:\n1. payment-gateway pods restarted due to memory spike when logging full Stripe webhook payloads at INFO level (a debugging flag left enabled during an earlier incident). When pods were down, Stripe retries built up.\n2. fulfillment-dispatcher subscribed to internal topic `orders.paid` and used `order_id` as a message payload, but it did not enforce idempotency. When checkout-service processed “paid” transition twice for the same order (due to a race during recovery), it emitted `orders.paid` twice.\n\nImportant nuance: Stripe duplicate webhooks alone should not have caused duplicate `orders.paid`. The immediate duplication happened because our own processor re-applied a transition in one edge case:\n- one worker processed the event and updated payment to `SUCCEEDED`\n- another worker retried after a transient DB timeout and, due to a missing unique constraint on `(order_id, transition)`, emitted `orders.paid` again even though payment state was already `SUCCEEDED`\n\nSo we had idempotency at Stripe event ingestion, but not across internal derived events.\n\n## Contributing factors\n- Logging configuration: `LOG_WEBHOOK_PAYLOADS=true` increased memory usage and GC pressure in Node, contributing to OOMKilled.\n- Missing DB constraint: no unique constraint preventing duplicate insertion into `order_transitions` table.\n- Downstream consumer assumption: fulfillment-dispatcher assumed “upstream events are exactly-once”, which is not true in our system.\n- Lack of replay tooling: we had to improvise dedupe scripts quickly.\n\n## Resolution\nImmediate:\n- Scaled fulfillment-dispatcher to 0 to stop further dispatch.\n- Canceled duplicate shipping labels via internal warehouse API (manual script).\n- Patched checkout-service to emit `orders.paid` only when a DB update changes payment_status from non-succeeded to succeeded.\n\nStabilization:\n- Disabled webhook payload logging and redeployed payment-gateway.\n- Reduced webhook concurrency temporarily to reduce DB timeouts.\n\n## Corrective actions\n1. Add unique constraint:\n```sql\nalter table order_transitions\n  add constraint order_transition_unique unique (order_id, transition_name);\n```\n2. Make fulfillment-dispatcher idempotent:\n- Store `dispatch_id` and ensure `(order_id)` unique.\n- If a duplicate arrives, log and ack without action.\n3. Add alert: payment-gateway memory usage and OOMKilled count.\n4. Update Stripe webhook runbook with explicit warning about payload logging and downstream idempotency.\n5. Add a “replay-safe” design guideline to engineering handbook: all consumers must assume at-least-once delivery.\n\n## What went well\n- We had visibility into duplicate dispatch quickly.\n- Pausing a downstream consumer was straightforward and limited blast radius.\n- No double charges occurred because Stripe ingestion was idempotent.\n\n## What went poorly\n- Our internal event stream did not follow consistent idempotency conventions.\n- Debug flags were not clearly separated from production-safe settings.\n\n## References\n- ADR-2024-12 Stripe webhooks as source of truth\n- RB-2026-01 Stripe webhook backlog\n- Onboarding: Payments flow"
    },
    {
      "doc_id": "cl-2025-09-prod-deploy",
      "title": "Checklist CL-2025-09: Production Deploy Checklist",
      "document_type": "checklist",
      "created_at": "2025-09-01",
      "content": "# Checklist CL-2025-09: Production Deploy Checklist (Core Services)\n\n## Purpose\nThis checklist is used when promoting a release to production for any of the core services: checkout-service, cart-service, search-api, payment-gateway. It is intentionally strict because many incidents were caused by “small” changes (env vars, migrations, cache format).\n\nIf you are doing an emergency rollback, skip to the rollback section.\n\n## Before you click “Promote”\n### 1) Confirm the artifact\n- Identify image tag (git sha) and release notes.\n- Ensure staging is running **the same image** you’re about to promote.\n- Verify CI passed on the merge commit (not a rebased SHA).\n\n### 2) Read the diff for “sharp edges”\nSpecifically scan for:\n- DB schema changes (alembic revisions)\n- Redis key formats or serialization changes\n- Elasticsearch mapping/template changes\n- Stripe webhook / payment changes\n- Any new env vars or config keys (pydantic Settings updates)\n\n### 3) Migration readiness\n- Confirm migration job exists and is configured to use Postgres primary, not PgBouncer.\n- In staging, ensure `alembic current == head`.\nIf you see migrations touching:\n- large tables\n- adding indexes\n- column type changes\nthen schedule deploy off-peak or pre-run migration.\n\n### 4) Backout plan\nWrite down (in the deploy issue):\n- “rollback to image X” command\n- who is on point to execute rollback\n- what metric you will watch to decide to rollback\n\n## During the promotion\n### 5) Start deploy\nUse the promote workflow. Do not apply manifests manually unless incident response requires it.\n\n### 6) Watch rollout status\n```bash\nkubectl -n ecommerce-prod rollout status deploy/<service>\nkubectl -n ecommerce-prod get pods -l app=<service>\n```\nIf rollout stalls > 5 minutes, follow RB-2025-06.\n\n### 7) Watch dashboards for 15 minutes\nMinimum dashboards:\n- Service latency + error rate (p50/p95, 4xx/5xx)\n- Postgres connections (`numbackends`)\n- Redis evictions (`redis_evicted_keys_total`) if service touches Redis\n- Stripe webhook queue depth if payment-gateway changed\n- Elasticsearch health if search-api or indexing-worker changed\n\n### 8) Smoke tests (realistic, not “ping”)\nPick 2–3 flows relevant to your change:\n- cart: add item, remove item, refresh, ensure ETag behavior is stable (no unexpected 409)\n- checkout: `/api/checkout/start` with idempotency key; retry request and ensure it returns same order_id\n- search: query common term and verify facets (unless in degraded mode)\n- payments: in staging, run Stripe test payment and confirm webhook processing\n\n## After deploy (stabilization)\n### 9) Confirm error budgets\nIf error budget burn accelerates (even if absolute 5xx seems small), pause and investigate.\n\n### 10) Check logs for new “unknowns”\nSearch for:\n- `ValidationError` (missing env)\n- `UndefinedColumn` (migration drift)\n- `redis.exceptions` spikes\n- `StripeSignatureVerificationError`\n- `index_not_found_exception`\n\n### 11) Redis-sensitive changes\nIf the change impacts cart caching:\n- Watch cache value sizes and evictions for at least 30 minutes.\n- Be ready to enable `CART_CACHE_BYPASS=true` quickly if evictions spike (RB-2026-02).\nRemember: Redis incidents can look like “random cart mismatch” and will be reported by product quickly.\n\n### 12) Document outcomes\nUpdate the deploy issue with:\n- start/end times\n- any anomalies (even if no incident)\n- links to dashboards\n\n## Emergency rollback\nRollback is the default response if:\n- 5xx spikes immediately after deploy\n- checkout conversion drops\n- signature verification fails for Stripe webhooks\n- DB schema mismatch errors appear\n\nCommands:\n```bash\nkubectl -n ecommerce-prod rollout undo deploy/checkout-service\nkubectl -n ecommerce-prod rollout undo deploy/cart-service\nkubectl -n ecommerce-prod rollout undo deploy/search-api\nkubectl -n ecommerce-prod rollout undo deploy/payment-gateway\n```\n\nAfter rollback:\n- capture logs and graphs\n- open a postmortem if user impact > 10 minutes or involves payments\n\n## Notes\n- “Small config changes” are real deploys. Treat them with the same checklist.\n- Do not enable verbose webhook payload logging in prod unless approved; it caused OOMKilled in PM-2025-08.\n\n## Canary and feature flag notes\n- If the change can be feature-flagged, ship code first with flag off, then enable gradually.\n- For risky changes (cache formats, payment state changes), deploy to 10% traffic in staging by scaling and validating mixed-version behavior before production."
    },
    {
      "doc_id": "cl-2025-10-weekly-ops",
      "title": "Checklist CL-2025-10: Weekly Operations Checklist",
      "document_type": "checklist",
      "created_at": "2025-10-06",
      "content": "# Checklist CL-2025-10: Weekly Operations Checklist (Platform + Core Services)\n\n## Purpose\nA weekly checklist to reduce “slow burn” outages: disk filling, queue backlogs, retention misconfig, expired secrets, and creeping DB load. This is not incident response; it’s preventative maintenance.\n\nTime budget: ~45–60 minutes. Run it early week (Mon/Tue) during business hours.\n\n## 1) Postgres health\n### Connections and slow queries\n- Check `numbackends` during peak.\n- Confirm no growing long-running queries.\n\nSQL snippets:\n```sql\nselect count(*) from pg_stat_activity;\nselect pid, now()-query_start as age, state, query\nfrom pg_stat_activity\nwhere state <> 'idle'\norder by age desc\nlimit 10;\n```\n\n### Vacuum / bloat\n- Identify tables with high dead tuples (common: `cart_items`, `stripe_events`).\nIf bloat is rising, schedule maintenance; don’t “vacuum full” during peak.\n\n## 2) PgBouncer sanity\n- Confirm services are connecting via PgBouncer (application_name patterns).\n- Review `cl_waiting` and `sv_active` for saturation signs.\n\nIf you see many direct connections to primary from app pods, open a ticket immediately; it is a common precursor to RB-2024-11 incidents.\n\n## 3) Redis: memory and eviction watch\n- Check maxmemory usage and eviction rate.\n- Review cache value size histograms for cart-service.\n\nIf `redis_evicted_keys_total` has sustained increases during normal traffic, investigate:\n- oversized values (recommendations accidentally cached)\n- TTL synchronization (missing jitter)\n- sudden traffic surges\n\nNote: Redis issues have caused checkout instability before (PM-2025-11).\n\n## 4) Elasticsearch retention and health\n- Cluster health should be green.\n- Disk usage should remain below low watermark.\n\nCommands:\n```bash\ncurl -s http://elasticsearch:9200/_cluster/health?pretty\ncurl -s http://elasticsearch:9200/_cat/indices/products-v3-*?h=index,store.size,docs.count&s=index\ncurl -s http://elasticsearch:9200/_cat/aliases/products_read?v\n```\n\nIf there are indices older than 14 days that still exist, ILM is likely broken. Create a ticket; don’t wait for disk watermark incidents.\n\n## 5) Stripe webhook hygiene\n- Check `stripe_webhook_queue_depth` baseline.\n- Review signature failure counts (`stripe_signature_fail_total`).\n\nIf signature failures are non-zero in prod:\n- verify no partial rollout of payment-gateway secrets\n- ensure ingress path for `/webhooks/stripe` hasn’t changed\n\n## 6) Background jobs and queues\n- indexing-worker Kafka lag should not be growing week-over-week.\n- refund-reconciler should have completed successfully in the last 24h.\n- stripe replay cronjob should be idle (manual runs are rare).\n\nIf refund-reconciler failed, support will notice days later; fix now.\n\n## 7) Secrets and certificates\n- Confirm Stripe keys are not near rotation deadline (quarterly).\n- Check ingress TLS cert expiry (our automation usually handles it, but verify).\n\nIf a secret was rotated, ensure all pods rolled; partial rotation causes intermittent failures that are hard to debug.\n\n## 8) Dashboards and alert noise\n- Review “top noisy alerts” and tune thresholds if they are flapping.\n- Ensure oncall dashboards still load quickly and panels point to valid metrics.\n\n## 9) Small housekeeping\n- Remove unused feature flags that are permanently on/off.\n- Confirm `READINESS_STRICT_DEPS` settings are consistent with current resilience strategy.\n\n## Output\nFile a weekly ops ticket with:\n- any anomalies\n- tickets opened\n- screenshots/links to dashboards if relevant\n\n## References\n- Runbooks: Redis evictions, Stripe webhook backlog, ES cluster red, Postgres connection exhaustion\n- Checklists: Production deploy\n\n## 10) CI/CD and registry hygiene\n- Verify GitHub Actions runners are healthy (no sustained queue of jobs).\n- Check that release images are being built and pushed (we've had “ImagePullBackOff” incidents from missing tags).\n- Confirm registry retention is deleting old sha tags to avoid storage creep.\n\nQuick sanity:\n- pick the last promoted SHA and confirm image exists in registry\n- verify `promote.yml` workflow success rate in the last week\n\n## 11) SLO / error budget review\n- Review weekly error budget burn for checkout-service and payment-gateway.\n- If burn > 30% in a week, open a reliability ticket even if no single SEV incident occurred.\nThis catches “death by a thousand cuts” (timeouts, small 5xx spikes).\n\n## 12) One-off checks (only if relevant)\n- After any Redis config change, confirm eviction policy remains `allkeys-lfu`.\n- After any ES reindex, confirm old indices were deleted and aliases point correctly."
    },
    {
      "doc_id": "kb-2026-02-runbook-redis-evictions",
      "title": "Runbook: Redis Evictions Causing Cart Staleness (cart-service, checkout-service)",
      "document_type": "runbook",
      "created_at": "2026-02-02",
      "content": "# Summary\nRedis eviction spikes (or Redis being temporarily unavailable) can cause carts to appear stale, totals to mismatch between cart-service and checkout-service, and a surge of 409/412 responses during checkout. This runbook covers detection, mitigation, and recovery.\n\n# Affected components\n- **cart-service** (FastAPI, python 3.11, uvicorn)\n- **checkout-service** (FastAPI)\n- **search-api** (indirect: recommendations in cart)\n- **Redis** (clustered, 3 masters + 3 replicas, Redis 7.2.4)\n- **Postgres** (primary source of truth for cart items)\n\n# Symptoms\n## User-facing\n- Cart shows missing items after refresh\n- Checkout fails with:\n  - `409 CART_VERSION_MISMATCH`\n  - `412 PRECONDITION_FAILED`\n- Totals change between cart page and checkout confirmation\n\n## Service metrics\n- cart-service: increased `redis_cache_miss_total`, `redis_errors_total`\n- checkout-service: increased `cart_fetch_fail_total`, `http_409_total`\n- Redis: `evicted_keys` rising, `used_memory` near `maxmemory`\n\n# Quick triage\n## 1) Confirm incident scope\n- Grafana dashboard: **Platform / Redis / Cache Health**\n  - Look at: `evicted_keys`, `connected_clients`, `keyspace_hits`, `keyspace_misses`\n- Service dashboards:\n  - **cart-service / Latency & Errors**\n  - **checkout-service / Checkout funnel**\n\n## 2) Check recent deploys\n- GitHub Actions: last successful deployment to prod for cart-service/checkout-service.\n- If within the last 30 minutes: suspect a regression in cache keying or TTL.\n\n## 3) Validate Redis connectivity from a pod\n`bash\nkubectl -n ecommerce-prod exec -it deploy/cart-service -- sh\npython -c \"import redis; r=redis.Redis(host='redis.ecommerce-prod.svc', port=6379, socket_connect_timeout=1); print(r.ping())\"\n`\nExpected: `True`. If timeout/connection error: go to **Mitigation A**.\n\n# Mitigation\n## Mitigation A: Redis unavailable or high error rate\n1. Enable **cache bypass** for cart reads in cart-service.\n   - Feature flag: `CART_CACHE_BYPASS=true`\n   - Set via config map `cart-service-config` (requires rollout)\n`bash\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_BYPASS=true\nkubectl -n ecommerce-prod rollout status deploy/cart-service\n`\n2. Confirm cart-service is reading from Postgres only.\n   - Look for log line: `cache=disabled source=postgres`\n3. Watch Postgres load.\n   - If Postgres CPU > 75% or p95 query latency > 200ms, throttle traffic (see **Mitigation D**).\n\n## Mitigation B: Evictions due to maxmemory\n1. Confirm maxmemory policy:\n   - Expected: `allkeys-lfu` (ADR-2025-03-redis-lfu)\n   - If `volatile-ttl` or `noeviction`, notify Infra.\n2. Reduce memory pressure by expiring large keys.\n   - Cart keys pattern: `cart:{user_id}:v{n}`\n`bash\nkubectl -n ecommerce-prod exec -it statefulset/redis-master -- redis-cli --scan --pattern 'cart:*' | head -n 200 | xargs -n 50 redis-cli DEL\n`\n   - Edge case: this will force users to reload carts from Postgres and rehydrate cache. Expect increased DB reads.\n3. Temporarily reduce cache TTL for carts:\n   - Env var: `CART_CACHE_TTL_SECONDS=120` (default 900)\n\n## Mitigation C: Version mismatch storms (409)\n- checkout-service uses optimistic concurrency:\n  - It passes `If-Match: cart_etag` to cart-service.\n- If etags are derived from Redis value and Redis evicts keys, etag may reset.\n1. Enable stable etag derivation from Postgres:\n   - Feature flag: `CART_ETAG_SOURCE=postgres`\n2. Rollout checkout-service.\n\n## Mitigation D: Traffic shaping\nIf DB pressure becomes the primary risk:\n1. Apply rate-limit at ingress for `/api/cart/*` and `/api/checkout/*`.\n2. Prefer 429 over cascading failures.\n\n# Recovery\n## Validate after mitigation\n- cart-service error rate < 1%\n- checkout funnel recovers (conversion within 10% of baseline)\n- Redis `evicted_keys` flat for 10+ minutes\n\n## Re-enable cache (if bypass was set)\n- Only after Redis memory and error rate stabilize.\n`bash\nkubectl -n ecommerce-prod set env deploy/cart-service CART_CACHE_BYPASS-\n`\n\n# Edge cases / pitfalls\n- **Ghost carts**: if a user has multiple sessions, stale local storage can re-submit old cart_id; expect `404 CART_NOT_FOUND`.\n- **Large carts** (> 200 items): serialized cart payload can exceed 1MB; Redis ops may exceed `proto-max-bulk-len` if misconfigured. Symptom: `ERR Protocol error: invalid bulk length`.\n- **Clock drift** on app nodes can break TTL-based reconciliation. Check node time sync if only some pods are affected.\n\n# Post-incident actions\n- Create a ticket to verify maxmemory policies and key sizes.\n- Add alert: `increase(redis_evicted_keys_total[5m]) > 0`.\n- Reference related postmortem: **PM-2025-11-redis-oom**.\n"
    },
    {
      "doc_id": "kb-2026-01-runbook-stripe-webhook-backlog",
      "title": "Runbook: Stripe Webhook Backlog (payment-gateway, checkout-service)",
      "document_type": "runbook",
      "created_at": "2026-01-18",
      "content": "# Summary\nStripe webhook delivery delays can cause orders to remain in `PAYMENT_PENDING`, duplicate fulfillment attempts, or missing refunds. This runbook covers diagnosing webhook backlog, verifying signature validation, and replaying events safely.\n\n# Services and dependencies\n- **payment-gateway** (legacy Node.js 18.19, Express)\n- **checkout-service** (FastAPI)\n- **Postgres** (orders, payments tables)\n- **Redis** (idempotency cache)\n- **Stripe** (API version pinned to 2023-10-16 in payment-gateway)\n\n# Symptoms\n## User-facing\n- Order confirmation page shows pending state for > 2 minutes\n- Refund initiated but status stays `REQUESTED`\n\n## Metrics\n- payment-gateway: `stripe_webhook_queue_depth` increasing\n- payment-gateway: `http_5xx_total` spikes on `/webhooks/stripe`\n- checkout-service: `orders_payment_pending_total` elevated\n\n# Immediate checks\n## 1) Stripe dashboard\n- Check recent webhook attempts for endpoint `https://api.company.tld/webhooks/stripe`.\n- Look for status codes:\n  - `400 invalid_signature`\n  - `429 rate_limited`\n  - `500 internal_error`\n\n## 2) payment-gateway logs\nSearch for:\n- `StripeSignatureVerificationError: No signatures found matching the expected signature for payload`\n- `UnhandledPromiseRejection: Error: connect ETIMEDOUT postgres`\n\n## 3) Validate secret and clock drift\nSignature verification is time-sensitive. If nodes drift, Stripe will still send but our code rejects.\n- Confirm env var: `STRIPE_WEBHOOK_SECRET` matches the one in Stripe dashboard.\n- Check time sync on nodes (Kubernetes node status + NTP).\n\n# Mitigation\n## A) If invalid_signature (400)\n1. Confirm the webhook secret in `payment-gateway` deployment.\n`bash\nkubectl -n ecommerce-prod describe deploy/payment-gateway | grep STRIPE_WEBHOOK_SECRET -n\n`\n2. Validate we are not behind a proxy that alters raw body.\n- payment-gateway must use `express.raw({ type: 'application/json' })` on webhook route.\n- If a recent change added `express.json()` globally before the webhook route, rollback.\n\n## B) If rate-limited (429) from our side\n- We throttle webhook handler with `WEBHOOK_MAX_CONCURRENCY`.\n1. Temporarily increase concurrency from 20 to 50.\n`bash\nkubectl -n ecommerce-prod set env deploy/payment-gateway WEBHOOK_MAX_CONCURRENCY=50\nkubectl -n ecommerce-prod rollout status deploy/payment-gateway\n`\n2. Ensure Postgres can handle the write burst.\n\n## C) If Postgres timeouts\n1. Switch payment-gateway to degraded mode: enqueue events only.\n- Env flag: `WEBHOOK_DEFER_DB_WRITES=true`\n- This stores minimal event data in Redis list `stripe:webhook:deferred`.\n2. Once DB is stable, run the replayer job:\n`bash\nkubectl -n ecommerce-prod create job --from=cronjob/stripe-webhook-replay stripe-webhook-replay-manual\n`\n\n# Safe replay / idempotency\nWe rely on idempotency at two layers:\n- Stripe event id: `evt_*` stored in Postgres `payments.stripe_event_id` (unique index)\n- Redis key: `stripe:evt:{id}` TTL 7 days\n\nIf replaying, verify unique constraint exists:\n`sql\n\\d payments;\n-- expect: unique (stripe_event_id)\n`\n\n# Validation steps\n## Confirm backlog draining\n- `stripe_webhook_queue_depth` should trend downward.\n- Stripe dashboard: attempts should return 2xx.\n\n## Confirm state transitions\nRun queries:\n`sql\nselect status, count(*) from orders where created_at > now() - interval '1 hour' group by 1;\nselect payment_status, count(*) from payments where created_at > now() - interval '1 hour' group by 1;\n`\nExpected: `PAYMENT_PENDING` not growing.\n\n# Edge cases\n- **Duplicate charge events**: Stripe can deliver the same event multiple times. Never assume once-only delivery.\n- **Out-of-order events**: `charge.refunded` can arrive before `payment_intent.succeeded` in rare retries. Our handler must tolerate this by checking current DB state.\n- **Large payloads**: Some events include expanded objects. If ingress has a 1MB limit, it may return 413, causing repeated retries.\n- **Partial deploy**: If only some pods have the correct webhook secret, failures appear intermittent.\n\n# Rollback guidance\nIf a deploy introduced signature failures or 500s:\n- Roll back payment-gateway to last known good image tag, typically `payment-gateway:1.38.2`.\n`bash\nkubectl -n ecommerce-prod rollout undo deploy/payment-gateway\n`\n\n# Related docs\n- ADR-2024-12: Stripe webhooks vs polling\n- PM-2025-08: Webhook retries causing duplicate fulfillment\n"
    },
    {
      "doc_id": "kb-2025-10-runbook-es-cluster-red",
      "title": "Runbook: Elasticsearch Cluster Red / Search Degradation (search-api, indexing-worker)",
      "document_type": "runbook",
      "created_at": "2025-10-07",
      "content": "# Summary\nThis runbook addresses Elasticsearch cluster health issues (yellow/red) leading to degraded search results, timeouts in search-api, and indexing lag. It assumes Elasticsearch 8.11.x deployed on Kubernetes with 3 master-eligible nodes and 6 data nodes.\n\n# Affected components\n- **search-api** (FastAPI)\n- **indexing-worker** (Python Celery worker consuming Kafka topic `catalog.product.changed`)\n- **Elasticsearch** (8.11.3)\n- **Postgres** (source of truth for product catalog)\n\n# Symptoms\n- search-api returns `503 SEARCH_UNAVAILABLE` or `504 upstream timeout`\n- p95 latency > 800ms on `/api/search`\n- Indexing lag: Kafka consumer group `indexing-worker` lag increases\n- Elastic health is `red` or `yellow`\n\n# Initial triage\n## 1) Check cluster health\n`bash\nkubectl -n ecommerce-prod port-forward svc/elasticsearch 9200:9200\ncurl -s http://localhost:9200/_cluster/health?pretty\n`\nKey fields:\n- `status`: green/yellow/red\n- `unassigned_shards`\n\n## 2) Check shard allocation explain\n`bash\ncurl -s -X GET http://localhost:9200/_cluster/allocation/explain?pretty\n`\nLook for:\n- `NODE_LEFT`\n- `disk_threshold`\n- `shards_limit`\n\n## 3) search-api errors\nCommon error snippet:\n- `elasticsearch.exceptions.ConnectionTimeout: Connection timed out`\n- `TransportError(429, 'es_rejected_execution_exception', ...)`\n\n# Mitigation paths\n## A) Disk watermarks (most common)\nIf allocation explain shows disk thresholds:\n1. Check disk usage on data nodes.\n2. Free space by deleting old indices:\n- Daily indices: `products-v3-YYYY.MM.DD`\n- Retention: 14 days\n`bash\ncurl -s -X GET 'http://localhost:9200/_cat/indices/products-v3-*?h=index,store.size,docs.count'\ncurl -s -X DELETE 'http://localhost:9200/products-v3-2025.09.*'\n`\n3. If retention is already enforced, check ILM policy `products-ilm-v3` for failures.\n\n## B) Unassigned primaries after node restart\n1. Confirm node count and pod status.\n`bash\nkubectl -n ecommerce-prod get pods -l app=elasticsearch\n`\n2. If a data node is crashlooping, identify OOM:\n- Check events: `OOMKilled`\n- Consider increasing memory limits (typical: 8Gi -> 12Gi)\n\n## C) Threadpool rejections (429)\nIf you see `es_rejected_execution_exception`:\n1. Reduce search concurrency by enabling circuit breaker in search-api:\n- Env var: `SEARCH_MAX_CONCURRENCY=40` (default 80)\n2. Temporarily disable expensive features:\n- `SEARCH_ENABLE_AGGS=false`\n- `SEARCH_ENABLE_SPELLCHECK=false`\n3. Validate CPU saturation on data nodes.\n\n## D) Indexing runaway\nIf indexing-worker is flooding ES:\n1. Pause consumer group by scaling workers to 0.\n`bash\nkubectl -n ecommerce-prod scale deploy/indexing-worker --replicas=0\n`\n2. Let ES recover.\n3. Resume with lower batch size:\n- `INDEXING_BULK_SIZE=250` (default 1000)\n\n# Recovery\n## Validate green state\n- Cluster health green for 10+ minutes\n- search-api p95 < 250ms\n- No unassigned shards\n\n## Re-enable features\nUndo any temporary flags gradually.\n\n# Edge cases\n- **Mapping explosions**: a product attribute field can become dynamic and create thousands of fields. Symptom: `Limit of total fields [1000] has been exceeded`.\n  - Fix: block dynamic mapping in template `products-template-v3`.\n- **Analyzer mismatch after deploy**: search-api might query a field that changed name. Symptom: `query_shard_exception: failed to find field`.\n  - Fix: deploy index alias compatibility or rollback.\n- **Split brain warnings**: if master pods flap, you may see `master_not_discovered_exception`. In that case, do not force allocate shards until masters stabilize.\n\n# Useful commands\n- Cat shards:\n`bash\ncurl -s 'http://localhost:9200/_cat/shards?v'\n`\n- Hot threads:\n`bash\ncurl -s 'http://localhost:9200/_nodes/hot_threads?pretty'\n`\n\n# Related docs\n- ADR-2025-02: Index aliasing strategy for products\n- PM-2025-12: ES disk watermark incident\n"
    },
    {
      "doc_id": "kb-2025-06-runbook-k8s-rollout-stuck",
      "title": "Runbook: Kubernetes Rollout Stuck / CrashLoopBackOff (FastAPI services)",
      "document_type": "runbook",
      "created_at": "2025-06-21",
      "content": "# Summary\nA deployment rollout can stall due to readiness probe failures, CrashLoopBackOff, or image pull errors. This runbook focuses on FastAPI services (checkout-service, cart-service, search-api) in `ecommerce-prod`.\n\n# Common causes\n- Wrong env vars or missing secrets\n- DB migration mismatch (alembic head not applied)\n- Redis/Elasticsearch DNS issues\n- Readiness probe misconfigured (path or port)\n- OOMKilled due to increased memory usage\n\n# Triage checklist\n## 1) Identify rollout status\n`bash\nkubectl -n ecommerce-prod rollout status deploy/checkout-service\nkubectl -n ecommerce-prod get pods -l app=checkout-service -o wide\n`\nIf it says `waiting for rollout to finish`, inspect failing pods.\n\n## 2) Describe the pod\n`bash\nkubectl -n ecommerce-prod describe pod <pod-name>\n`\nLook for:\n- `ImagePullBackOff`\n- `OOMKilled`\n- `Readiness probe failed`\n- `Back-off restarting failed container`\n\n## 3) Check logs\n`bash\nkubectl -n ecommerce-prod logs <pod-name> --previous\nkubectl -n ecommerce-prod logs <pod-name>\n`\nCommon FastAPI startup failures:\n- `sqlalchemy.exc.OperationalError: could not connect to server: Connection refused`\n- `alembic.util.exc.CommandError: Can't locate revision identified by '...'\n- `pydantic_core.*pydantic_core.ValidationError: 1 validation error for Settings`\n\n# Mitigation by scenario\n## A) Readiness probe failures\nExpected readiness path: `/health/ready`(returns 200 JSON:`{ 'status': 'ok' }`).\n1. Verify probe config:\n- port `8080`\n- initialDelaySeconds `10`\n- timeoutSeconds `2`\n2. Exec into pod and curl locally:\n```bash\nkubectl -n ecommerce-prod exec -it <pod-name> -- sh\nwget -qO- http://127.0.0.1:8080/health/ready\n```\nEdge case: service starts but waits for dependent check (Postgres/Redis). If Redis is down, readiness will fail by design unless `READINESS_STRICT_DEPS=false`.\n\n## B) Missing secrets/config\nSettings are validated at startup. If a required var is missing, pydantic raises.\n1. Compare env var set with expected list in `docs/config/service-env.md`.\n2. Confirm secret exists:\n```bash\nkubectl -n ecommerce-prod get secret checkout-service-secrets\n```\n3. If secret key renamed, roll back or patch.\n\n## C) DB migration mismatch\nIf the container expects a new schema but migrations weren’t applied:\n- Symptom: app crashes on import with missing column.\n- Example: `psycopg.errors.UndefinedColumn: column orders.payment_provider does not exist`\nActions:\n1. Stop the rollout by scaling replicas to previous version (or rollback):\n```bash\nkubectl -n ecommerce-prod rollout undo deploy/checkout-service\n```\n2. Apply migrations via the migration job:\n```bash\nkubectl -n ecommerce-prod create job --from=cronjob/checkout-migrations checkout-migrations-manual\n```\n3. Re-deploy.\n\n## D) OOMKilled\n1. Identify memory usage increase via Grafana: **K8s / Pod Memory**.\n2. Temporarily raise limits:\n- Typical for checkout-service: requests 300Mi, limits 800Mi.\n3. If recent change enabled debug logging or loaded large models, rollback.\n\n## E) ImagePullBackOff\n1. Check image tag exists in registry.\n2. Confirm imagePullSecret is present.\n3. If GitHub Actions pushed to wrong repo, fix pipeline.\n\n# Recovery validation\n- Rollout completes.\n- Error rate stable.\n- p95 latency returns to baseline.\n\n# Edge cases\n- **Partial rollouts**: some pods ready, some failing. This can cause mixed behavior (new code path expects new Redis keys). Prefer rollback quickly.\n- **Stuck terminating**: old ReplicaSet pods hanging due to long shutdown. Increase `terminationGracePeriodSeconds`or fix`lifespan`handlers.\n- **Readiness vs liveness confusion**: do not point liveness at external dependency checks; use a local-only endpoint.\n\n# Related docs\n- Checklist: Production deploy checklist (CL-2025-09-prod-deploy)\n- Postmortem: PM-2025-07-migrations-caused-outage\n"
    },
    {
      "doc_id": "kb-2024-11-runbook-postgres-connection-exhaustion",
      "title": "Runbook: Postgres Connection Exhaustion (too many clients) Across Services",
      "document_type": "runbook",
      "created_at": "2024-11-29",
      "content": "# Summary\nPostgres connection exhaustion manifests as widespread 500s with`FATAL: sorry, too many clients already`. This often occurs after a deploy that increases worker concurrency or when a connection pool misconfiguration disables pooling.\n\n# Services\n- checkout-service (FastAPI + SQLAlchemy 2.0)\n- cart-service (FastAPI)\n- search-api (FastAPI; reads catalog DB for filters)\n- payment-gateway (Node; writes payments and refunds)\n\n# Symptoms\n- Elevated 5xx across multiple services\n- Postgres metrics:\n  - `pg_stat_activity`shows connections near`max_connections`(prod default 600)\n  - CPU may be normal but connections saturate\n- App logs:\n  -`psycopg.OperationalError: connection failed: FATAL: sorry, too many clients already`\n\n# Triage\n## 1) Confirm max connections and current usage\n```sql\nshow max_connections;\nselect count(*) from pg_stat_activity;\nselect usename, application_name, count(*) from pg_stat_activity group by 1,2 order by 3 desc;\n```\nExpected application_name conventions:\n- `checkout-service@<pod>`\n- `cart-service@<pod>`\n- `payment-gateway@<pod>`\n\n## 2) Identify runaway clients\nLook for:\n- many idle in transaction\n- long-running queries holding connections\n```sql\nselect pid, usename, application_name, state, now()-xact_start as xact_age, query\nfrom pg_stat_activity\nwhere state <> 'idle'\norder by xact_age desc\nlimit 20;\n```\n\n# Mitigation\n## A) Reduce app concurrency quickly\nFor FastAPI services, gunicorn/uvicorn workers can multiply pools.\n1. Lower workers:\n- `WEB_CONCURRENCY=2`(temporary)\n```bash\nkubectl -n ecommerce-prod set env deploy/checkout-service WEB_CONCURRENCY=2\n```\n2. Lower async DB pool size:\n-`DB_POOL_SIZE=10`(default 30)\n-`DB_MAX_OVERFLOW=5`(default 20)\n\n## B) Enable pgbouncer (if disabled)\nWe run pgbouncer in transaction pooling mode.\n- Service DNS:`pgbouncer.ecommerce-prod.svc:6432`\n- Env var: `DATABASE_URL=postgresql+psycopg://...@pgbouncer:6432/app`\nIf a deploy pointed services at primary directly, revert.\n\n## C) Kill idle-in-transaction sessions (last resort)\nOnly after identifying offenders.\n```sql\nselect pg_terminate_backend(pid)\nfrom pg_stat_activity\nwhere state='idle in transaction' and now()-xact_start > interval '2 minutes';\n```\n\n## D) Protect primary\nIf saturation continues, apply rate limits to checkout endpoints and return 429.\n\n# Recovery\n- Connections drop below 70% of max\n- Error rate returns to baseline\n- Verify pgbouncer stats:\n```sql\nshow pools;\nshow stats;\n```\n\n# Edge cases\n- **Connection leaks**: if SQLAlchemy sessions are not closed on exception paths. Look for code changes around `async with session.begin()`.\n- **Background workers**: indexing-worker or refund-reconciler might have higher concurrency than web pods.\n- **Node service pooling**: payment-gateway uses `pg`module; ensure`max: 20`is set. Without it, Node can open unbounded connections under load.\n\n# Related\n- ADR-2024-08: Adopt pgbouncer transaction pooling\n- Checklist: CL-2025-09-prod-deploy (verify DB pool settings)\n"
    },
    {
      "doc_id": "kb-2024-12-adr-stripe-webhooks",
      "title": "ADR-2024-12: Stripe Webhooks as Source of Truth for Payment State",
      "document_type": "adr",
      "created_at": "2024-12-15",
      "content": "# Status\nAccepted\n\n# Context\nWe integrate Stripe for card payments and refunds. Historically, checkout-service polled Stripe for PaymentIntent status, which caused:\n- Increased API cost and throttling during traffic spikes\n- Race conditions when polling lagged behind actual status\n- Complex retry logic across services\n\nWe already have a legacy Node **payment-gateway** that receives Stripe webhook events, but its handler was previously treated as best-effort and not authoritative.\n\n# Decision\nMake **Stripe webhooks** the authoritative source of payment state. Specifically:\n- **payment-gateway** will validate webhook signatures and persist raw event payloads (minimally) in Postgres`stripe_events`.\n- **checkout-service** will update `payments.payment_status`based on processed events only.\n- Polling will remain as a fallback only for reconciliation jobs (daily) and for manual support tools.\n\n# Technical details\n## Stripe API version\nPin Stripe API version in payment-gateway to`2023-10-16`to keep event shape stable.\n\n## Webhook endpoint\n- Route:`POST /webhooks/stripe`\n- Must use raw body to validate signature.\n\n## Signature verification\n- Env var: `STRIPE_WEBHOOK_SECRET`\n- Reject if signature invalid. Log sample:\n  - `StripeSignatureVerificationError: No signatures found matching the expected signature`\n\n## Idempotency\n- Store `event.id`(e.g.,`evt*...`) in `stripe_events.event_id`(unique).\n- Before processing, check existence.\n\n## Processing pipeline\n1. Receive event and persist to`stripe_events`.\n2. Enqueue internal job `process_stripe_event(event_id)`.\n3. Update `payments`and`orders`tables in a transaction.\n\nSchema (proposed):\n```sql\ncreate table stripe_events (\n  event_id text primary key,\n  type text not null,\n  created_at timestamptz not null default now(),\n  payload jsonb not null,\n  processed_at timestamptz\n);\n```\n\n# Consequences\n## Positive\n- Lower Stripe API usage and fewer throttling errors\n- Single consistent state transition mechanism\n- Better auditability (raw events stored)\n\n## Negative / risks\n- If webhook handler breaks, payment state stalls (orders remain pending).\n- Requires strict operational runbook (see RB-2026-01 stripe webhook backlog).\n- Event order is not guaranteed; code must handle out-of-order events.\n\n# Alternatives considered\n## A) Continue polling\nRejected due to cost and complexity.\n\n## B) Stripe events to Kafka\nDeferred. Would add infrastructure and operational overhead; revisit when we migrate payment-gateway to Python.\n\n# Notes\nThis ADR assumes Postgres availability for event persistence. If Postgres is down, payment-gateway should return 500 to force Stripe retries (do not ack without persistence).\n"
    },
    {
      "doc_id": "kb-2025-03-adr-redis-cart-cache",
      "title": "ADR-2025-03: Redis as Primary Cache for Carts (LFU eviction policy)",
      "document_type": "adr",
      "created_at": "2025-03-11",
      "content": "# Status\nAccepted\n\n# Context\nCart reads are among our highest QPS endpoints. Postgres load during peak sales events led to elevated p95 (> 450ms) on cart-service. We need a low-latency cache, and we already operate Redis.\n\nWe previously cached carts in-process, but:\n- Pods scale horizontally, cache warmup is inconsistent\n- Deploys flush in-process caches\n- Memory usage was unpredictable\n\n# Decision\nUse Redis as the primary cache for cart payloads in **cart-service**, with Postgres as the source of truth.\n- Cache key format:`cart:{user_id}:v`\n- Cache TTL: 900 seconds\n- Eviction policy: `allkeys-lfu`\n- Payload: msgpack-encoded cart model (v2)\n\n# Technical details\n## Redis config\nBaseline config snippet:\n```conf\nmaxmemory 18gb\nmaxmemory-policy allkeys-lfu\nactivedefrag yes\ntimeout 0\n```\n\n## cart-service behavior\n1. Read-through cache:\n- GET `/api/cart/{user_id}`:\n  - Try Redis, if hit return cached cart.\n  - If miss, load from Postgres, write to Redis.\n2. Write-through on mutation:\n- POST `/api/cart/{user_id}/items`:\n  - Update Postgres transactionally.\n  - Publish event `cart.updated`.\n  - Update Redis key.\n\n## ETag / concurrency\ncheckout-service uses ETag for optimistic concurrency.\n- `ETag`derived from`cart.version`and item hashes.\n- Response header:`ETag: W/{etag}`\n\n## Failure mode\nIf Redis is unavailable:\n- Fall back to Postgres.\n- Set header `X-Cache: bypass`.\n\n# Consequences\n## Positive\n- Reduced Postgres load during peaks\n- Faster cart page loads\n\n## Negative / risks\n- Redis eviction under memory pressure can cause cache churn.\n- If ETag derivation depends on cached payload and keys are evicted, version mismatch behavior can worsen.\n\n# Alternatives\n## A) Store carts fully in Redis (source of truth)\nRejected: higher risk of data loss and complex durability.\n\n## B) Use Postgres only with aggressive indexing\nRejected: cost and limited latency improvement.\n\n# Follow-ups\n- Add alerting on Redis evictions.\n- Document incident response (see RB-2026-02 Redis evictions).\n\n# Note\nThis ADR prioritizes performance. If we observe eviction-driven incidents, we may need to revisit TTL, key size limits, or move to a smaller cached representation.\n"
    },
    {
      "doc_id": "kb-2025-02-adr-es-aliasing",
      "title": "ADR-2025-02: Elasticsearch Index Aliasing and Zero-Downtime Reindexing for Products",
      "document_type": "adr",
      "created_at": "2025-02-06",
      "content": "# Status\nAccepted\n\n# Context\nWe need to evolve product search mappings (new analyzers, fields) without downtime. Directly updating mappings is limited, and reindexing can take hours for 30M docs.\n\n# Decision\nAdopt an index aliasing strategy:\n- Write alias: `products_write`\n- Read alias: `products_read`\n- Concrete indices: `products-v3-YYYY.MM.DD`\n\nReindex flow:\n1. Create new index with updated template.\n2. Bulk reindex from Postgres snapshot (or existing index).\n3. Atomically switch read alias.\n4. Switch write alias after dual-write validation.\n\n# Technical details\n## Index templates\nTemplate name: `products-template-v3`\n- Dynamic mapping: disabled for `attributes.*`to avoid field explosion.\n-`total_fields.limit`: 1000\n\nExample template snippet:\n```json\n{\n  \"index_patterns\": [\"products-v3-*\"]\n}\n```\n\n## Dual write\nindexing-worker publishes to both old and new indices for 30 minutes.\n- Env var: `INDEX_DUAL_WRITE=true`\n\n## Aliasing commands\n```bash\ncurl -X POST localhost:9200/_aliases -H 'Content-Type: application/json' -d '{\n  \"actions\": [\n    {\"remove\": {\"alias\": \"products_read\", \"index\": \"products-v3-2025.01.30\"}},\n    {\"add\": {\"alias\": \"products_read\", \"index\": \"products-v3-2025.02.06\"}}\n  ]\n}'\n```\n\n# Consequences\n## Positive\n- Zero downtime mapping changes\n- Safer rollback (repoint alias)\n\n## Negative\n- Higher storage usage during reindex windows\n- Operational complexity during dual write\n\n# Alternatives\n- Use a single rolling index with ILM only: rejected; mapping changes still require reindex.\n\n# Edge cases\n- If a node is under disk watermark, alias switch won’t fix unassigned shards.\n- If search-api uses field names removed in new mapping, queries can fail with `failed to find field`.\n\n# References\n- Runbook: RB-2025-10 ES cluster red\n"
    },
    {
      "doc_id": "kb-2024-08-adr-pgbouncer",
      "title": "ADR-2024-08: Introduce PgBouncer Transaction Pooling for Postgres",
      "document_type": "adr",
      "created_at": "2024-08-22",
      "content": "# Status\nAccepted\n\n# Context\nWe operate multiple web services and workers, each with its own DB pool. Under load, the sum of pools exceeds Postgres capacity, resulting in incidents with `too many clients already`.\n\n# Decision\nAdopt PgBouncer in **transaction pooling** mode for all services.\n- Postgres remains primary.\n- Services connect to PgBouncer at port 6432.\n\n# Technical details\n## PgBouncer settings\n- `pool_mode = transaction`\n- `max_client_conn = 5000`\n- `default_pool_size = 50`\n- `reserve_pool_size = 10`\n- `server_idle_timeout = 60`\n\n## Service configuration\n- Replace direct DB host with `pgbouncer.ecommerce-prod.svc`\n- SQLAlchemy async:\n  - `pool_size=10`\n  - `max_overflow=5`\n\n## Limitations\nTransaction pooling breaks session-level features:\n- prepared statements\n- temp tables across transactions\n- session settings (must be set per transaction)\n\nFor services requiring session features (rare), we allow direct connections behind an exception process.\n\n# Consequences\n- Lower connection pressure on Postgres\n- Additional operational component (PgBouncer)\n\n# Rollout plan\n1. Deploy PgBouncer and validate health.\n2. Migrate services one by one.\n3. Add alert on Postgres connection usage.\n\n# Edge cases\n- Node payment-gateway must disable statement-level prepared queries if it assumes session pooling.\n- Alembic migrations should run against primary Postgres, not PgBouncer.\n\n# References\n- Runbook: RB-2024-11 Postgres connection exhaustion\n"
    },
    {
      "doc_id": "kb-2026-01-onboarding-fastapi-services",
      "title": "Onboarding Guide: Working on FastAPI Microservices (checkout-service, cart-service, search-api)",
      "document_type": "onboarding",
      "created_at": "2026-01-05",
      "content": "# Overview\nThis guide gets you from zero to running our core FastAPI services locally with Docker and a minimal Kubernetes dev setup.\n\n# Repos and services\n- `checkout-service/`(Python 3.11, FastAPI 0.110)\n-`cart-service/`(Python 3.11)\n-`search-api/`(Python 3.11)\n-`infra/`(docker-compose, k8s manifests)\n- Legacy:`payment-gateway/`(Node 18)\n\n# Local prerequisites\n- Docker Desktop 4.27+\n- Python 3.11.7\n- Poetry 1.8+\n- kubectl 1.29+\n\n# Quick start (docker-compose)\nFrom`infra/`:\n```bash\ndocker compose up -d postgres redis elasticsearch\n```\nDefaults:\n- Postgres: `localhost:5432`, db `ecommerce`, user `app`, password `app`\n- Redis: `localhost:6379`\n- Elasticsearch: `localhost:9200`(single-node)\n\n# Running checkout-service\n```bash\ncd checkout-service\npoetry install\nexport DATABASE_URL=postgresql+psycopg://app:app@localhost:5432/ecommerce\nexport REDIS_URL=redis://localhost:6379/0\npoetry run uvicorn app.main:app --reload --port 8081\n```\nHealth endpoints:\n-`/health/live`\n- `/health/ready`\n\nCommon startup errors:\n- Missing env var:\n  - `pydantic_core._pydantic_core.ValidationError: Settings.STRIPE_API_KEY Field required`\n- Wrong DB URL:\n  - `psycopg.OperationalError: connection failed: Connection refused`\n\n# DB migrations\nWe use Alembic.\n```bash\ncd checkout-service\npoetry run alembic upgrade head\n```\nRule: migrations run against **primary Postgres**, not PgBouncer.\n\n# Running cart-service\n```bash\ncd cart-service\npoetry install\nexport CART_CACHE_TTL_SECONDS=900\npoetry run uvicorn app.main:app --reload --port 8082\n```\nEdge case: if Redis is down and `READINESS_STRICT_DEPS=true`, readiness fails. For local dev, set `READINESS_STRICT_DEPS=false`.\n\n# Running search-api\n```bash\ncd search-api\npoetry install\nexport ELASTICSEARCH_URL=http://localhost:9200\npoetry run uvicorn app.main:app --reload --port 8083\n```\nIf you see:\n- `failed to find field`errors: your local ES index mapping is stale. Recreate:\n```bash\ncurl -X DELETE http://localhost:9200/products-v3-local\npython scripts/create_index.py\n```\n\n# Testing\n- Unit tests:`poetry run pytest -q`\n- Integration tests require docker-compose dependencies.\n\n# Internal conventions\n- Service names in logs: `service=<name> env=<env> commit=<sha>`\n- Error responses:\n  - JSON: `{ 'error_code': '...', 'message': '...' }`\n- Timeouts:\n  - All outbound calls must set connect + read timeouts.\n\n# Debugging tips\n- Use `X-Request-Id`header; services propagate it.\n- For DB issues, check`application_name` in Postgres (`checkout-service@local`).\n- For Redis cache behavior, set `LOG_CACHE_KEYS=true`in cart-service (do not enable in prod).\n\n# Next steps\n- Read runbooks: RB-2026-02 Redis evictions, RB-2026-01 Stripe webhook backlog\n- Read ADRs: ADR-2025-03 Redis cart cache, ADR-2024-12 Stripe webhooks\n"
    },
    {
      "doc_id": "kb-2025-04-onboarding-payments",
      "title": "Onboarding Guide: Payments Flow (Stripe, payment-gateway, checkout-service)",
      "document_type": "onboarding",
      "created_at": "2025-04-03",
      "content": "# Overview\nPayments touch multiple services and have strict idempotency and audit requirements. This guide explains the current flow, common pitfalls, and how to test safely.\n\n# Services\n- **checkout-service** (creates orders, initiates payment intents)\n- **payment-gateway** (Node legacy; owns Stripe keys and webhook ingestion)\n- **refund-reconciler** (cronjob; reconciles refunds daily)\n\n# Data model (Postgres)\nKey tables:\n-`orders(id, status, total_amount, currency, created_at)`\n- `payments(id, order_id, provider, payment_status, stripe_payment_intent_id, stripe_event_id)`\n- `stripe_events(event_id, type, payload, processed_at)`\n\nConstraints:\n- `payments.stripe_event_id`has a unique index (prevents double-processing)\n\n# Happy path\n1. Client calls checkout-service:\n-`POST /api/checkout/start`\n2. checkout-service requests payment intent creation from payment-gateway:\n- internal endpoint: `POST /internal/stripe/payment_intents`\n3. payment-gateway creates PaymentIntent in Stripe (API version 2023-10-16).\n4. Client confirms payment via Stripe client SDK.\n5. Stripe sends webhook `payment_intent.succeeded`.\n6. payment-gateway persists event, enqueues processing.\n7. checkout-service updates payment/order state.\n\n# Local testing\n## Stripe CLI\nUse Stripe CLI to forward webhooks to local payment-gateway:\n```bash\nstripe listen --forward-to localhost:8090/webhooks/stripe\n```\nSet env:\n- `STRIPE_WEBHOOK_SECRET`from Stripe CLI output.\n\n## Common local failure\nIf you see`invalid_signature`, ensure the webhook route uses raw body parsing.\n\n# Idempotency rules\n- Stripe events are at-least-once.\n- All handlers must tolerate duplicates and out-of-order delivery.\n\nWe use:\n- Postgres uniqueness on event id\n- Redis TTL key: `stripe:evt:{event_id}`(7 days)\n\n# Operational notes\n- If Postgres is down, payment-gateway should not ack webhooks.\n- If Redis is down, we still rely on Postgres unique constraint (slower but correct).\n\n# Edge cases\n- Partial refunds: multiple`charge.refunded`events can occur.\n- Currency mismatch: Stripe may return amounts in minor units; ensure rounding is consistent.\n- 3DS flows:`requires_action`state can last minutes.\n\n# Where to look when things break\n- Runbook: RB-2026-01 Stripe webhook backlog\n- Postmortem: PM-2025-08 duplicate fulfillment\n- ADR: ADR-2024-12 Stripe webhooks source of truth\n"
    },
    {
      "doc_id": "kb-2024-09-onboarding-k8s-and-ci",
      "title": "Onboarding Guide: CI/CD and Kubernetes Basics for This Platform",
      "document_type": "onboarding",
      "created_at": "2024-09-14",
      "content": "# Overview\nThis guide covers how we build, test, and deploy services using GitHub Actions, Docker, and Kubernetes.\n\n# CI: GitHub Actions\nEach service has`.github/workflows/ci.yml`with jobs:\n-`lint`(ruff 0.4.x)\n-`test`(pytest)\n-`build`(docker buildx)\n-`push`(container registry)\n\nConventions:\n- Image tag:`<service>:<git-sha>`\n- Release tag: `<service>:v<semver>`\n\n# Deployment flow\n- Merges to `main`deploy to staging.\n- Manual promotion to prod via workflow`promote.yml`.\n\n# Kubernetes namespaces\n- `ecommerce-staging`\n- `ecommerce-prod`\n\n# Common kubectl commands\n- Rollout status:\n```bash\nkubectl -n ecommerce-prod rollout status deploy/cart-service\n```\n- Events:\n```bash\nkubectl -n ecommerce-prod get events --sort-by=.metadata.creationTimestamp | tail -n 50\n```\n\n# Config and secrets\n- ConfigMaps: non-sensitive config.\n- Secrets: Stripe keys, DB passwords.\n\nCommon failure:\n- Missing secret key -> pydantic validation errors at startup.\n\n# Observability\n- Prometheus scrapes `/metrics`.\n- Grafana dashboards are grouped by service.\n\n# Edge cases\n- Readiness probes can block rollout even if service is healthy locally.\n- If you accidentally point migrations at PgBouncer, you can get confusing errors.\n\n# Related docs\n- Runbook: RB-2025-06 rollout stuck\n- Checklist: CL-2025-09 prod deploy\n"
    },
    {
      "doc_id": "kb-2025-11-api-cart-service",
      "title": "API Spec: cart-service Public API (v1)",
      "document_type": "api_spec",
      "created_at": "2025-11-02",
      "content": "# Overview\ncart-service owns cart state and exposes endpoints for reading and mutating carts. Postgres is the source of truth; Redis is a performance cache (see ADR-2025-03).\n\nBase URL: `/api/cart`\nAuth: bearer token, required for all endpoints.\nIdempotency: mutation endpoints accept `Idempotency-Key`header.\n\n# Models\n## Cart\n-`cart_id`(uuid)\n-`user_id`(uuid)\n-`version`(int)\n-`items`(list)\n-`currency`(string)\n-`updated_at`(RFC3339)\n\n## CartItem\n-`sku`(string)\n-`qty`(int, 1..99)\n-`unit_price_minor`(int)\n\n# Endpoints\n## GET /api/cart/{user_id}\nReturns the current cart.\n\nHeaders:\n- Optional:`If-None-Match`(ETag)\n\nResponses:\n- 200: cart payload\n- 304: not modified\n- 404:`{ 'error_code': 'CART_NOT_FOUND' }`\n\nExample response:\n```json\n{\n  \"cart_id\": \"3b3b6f9e-0f2e-4f0c-9d8a-1b2a3c4d5e6f\",\n  \"user_id\": \"9a9a...\",\n  \"version\": 17,\n  \"currency\": \"USD\",\n  \"items\": [{\"sku\": \"SKU-123\", \"qty\": 2, \"unit_price_minor\": 1299}],\n  \"updated_at\": \"2025-11-02T10:12:30Z\"\n}\n```\n\n## POST /api/cart/{user_id}/items\nAdds or updates an item.\n\nHeaders:\n- `Idempotency-Key: <uuid>`\n- `If-Match: <etag>`(required)\n\nRequest:\n```json\n{ \"sku\": \"SKU-123\", \"qty\": 3 }\n```\n\nResponses:\n- 200: updated cart\n- 409:`{ 'error_code': 'CART_VERSION_MISMATCH' }`\n- 412: `{ 'error_code': 'PRECONDITION_FAILED' }`(missing/invalid If-Match)\n- 422:`{ 'error_code': 'INVALID_QTY' }`\n\nEdge cases:\n- qty 0 is rejected (use DELETE endpoint).\n- large carts (>200 items) may exceed payload size limits; clients should paginate using `GET /api/cart/{user_id}/items?cursor=...`(internal-only).\n\n## DELETE /api/cart/{user_id}/items/{sku}\nRemoves an item.\n\nResponses:\n- 200: updated cart\n- 404:`{ 'error_code': 'ITEM_NOT_FOUND' }`\n\n# Caching headers\n- cart-service returns:\n  - `ETag: W/<hash>`\n  - `Cache-Control: private, max-age=0`\n\n# Error format\nAll errors:\n```json\n{ \"error_code\": \"...\", \"message\": \"...\", \"request_id\": \"...\" }\n```\n\n# Operational notes\n- If Redis is unavailable, `X-Cache: bypass`is set.\n- If Redis evictions spike, users may see more 409s (see RB-2026-02).\n"
    },
    {
      "doc_id": "kb-2025-09-api-checkout-service",
      "title": "API Spec: checkout-service Public API (v2)",
      "document_type": "api_spec",
      "created_at": "2025-09-19",
      "content": "# Overview\ncheckout-service orchestrates checkout, order creation, and payment initiation via payment-gateway.\n\nBase URL:`/api/checkout`\nAuth: bearer token.\nIdempotency: required for order creation.\n\n# Endpoints\n## POST /api/checkout/start\nCreates an order draft and returns a payment intent client secret.\n\nHeaders:\n- `Idempotency-Key: <uuid>`\n\nRequest:\n```json\n{ \"user_id\": \"...\", \"cart_id\": \"...\", \"shipping_address_id\": \"...\" }\n```\n\nResponses:\n- 201:\n```json\n{ \"order_id\": \"...\", \"status\": \"PAYMENT_PENDING\", \"stripe_client_secret\": \"pi_..._secret_...\" }\n```\n- 409: `{ 'error_code': 'CART_VERSION_MISMATCH' }`\n- 422: `{ 'error_code': 'ADDRESS_INVALID' }`\n- 503: `{ 'error_code': 'PAYMENT_PROVIDER_UNAVAILABLE' }`\n\nEdge cases:\n- If cart-service returns 304 (not modified), checkout-service must still fetch cart totals from persisted snapshot to avoid mismatch.\n- If payment-gateway times out but Stripe created the intent, idempotency key must map to the same intent.\n\n## GET /api/checkout/orders/{order_id}\nReturns order status.\n\nResponses:\n- 200: order payload\n- 404: `{ 'error_code': 'ORDER_NOT_FOUND' }`\n\n## POST /api/checkout/orders/{order_id}/cancel\nCancels an unpaid order.\n\nResponses:\n- 200: cancelled\n- 409: `{ 'error_code': 'ORDER_ALREADY_PAID' }`\n\n# State machine\n- `DRAFT`->`PAYMENT_PENDING`->`PAID`->`FULFILLING`->`FULFILLED`\n- Failures:\n  - `PAYMENT_FAILED`\n  - `CANCELLED`\n\n# Error codes\n- `PAYMENT_PENDING_TIMEOUT`(order stuck > 10 minutes)\n-`PAYMENT_EVENT_DUPLICATE`(Stripe event already processed)\n\n# Observability\n-`checkout_start_total{status}`\n- `checkout_payment_pending_total`\n\n# Related docs\n- ADR-2024-12 Stripe webhooks\n- Runbook: RB-2026-01 Stripe webhook backlog\n"
    },
    {
      "doc_id": "kb-2025-05-api-search-api",
      "title": "API Spec: search-api Public API (v3) for Product Search",
      "document_type": "api_spec",
      "created_at": "2025-05-28",
      "content": "# Overview\nsearch-api provides product search backed by Elasticsearch. It exposes a stable query interface even as index mappings evolve (see ADR-2025-02).\n\nBase URL: `/api/search`\nAuth: optional for anonymous search; required for personalized boosts.\n\n# Endpoint\n## GET /api/search/products\nQuery params:\n- `q`(string)\n-`page`(int, default 1)\n-`page_size`(int, default 24, max 100)\n-`filters`(repeatable, e.g.,`brand:acme`)\n- `sort` (`relevance|price_asc|price_desc`)\n\nResponse 200:\n```json\n{\n  \"total\": 1234,\n  \"page\": 1,\n  \"page_size\": 24,\n  \"results\": [{\"sku\": \"SKU-123\", \"title\": \"...\", \"price_minor\": 1299}]\n}\n```\n\nErrors:\n- 400: `{ 'error_code': 'INVALID_FILTER' }`\n- 503: `{ 'error_code': 'SEARCH_UNAVAILABLE' }`\n\n# Timeouts and retries\n- search-api sets ES request timeout 150ms.\n- Retries are disabled to avoid amplifying load.\n\n# Edge cases\n- `q`empty: returns trending products (cached for 60s).\n- If ES returns 429`es_rejected_execution_exception`, search-api returns 503 with `retry_after_seconds: 2`.\n- If index alias missing (misconfig), error:\n  - `index_not_found_exception: no such index [products_read]`\n\n# Operational notes\n- Read alias: `products_read`\n- If cluster health is red, enable degraded mode:\n  - `SEARCH_ENABLE_AGGS=false`\n\n# Related docs\n- Runbook: RB-2025-10 ES cluster red\n- Postmortem: PM-2025-12 ES disk watermark incident\n"
    },
    {
      "doc_id": "kb-2025-11-postmortem-redis-oom",
      "title": "Postmortem: Redis OOM and Evictions Broke Checkout Consistency",
      "document_type": "postmortem",
      "created_at": "2025-11-30",
      "content": "# Incident summary\nOn 2025-11-29, Redis experienced sustained memory pressure leading to OOM behavior and heavy evictions. cart-service cache churn caused a surge in cart version mismatches during checkout, reducing conversion.\n\n# Impact\n- Duration: 47 minutes (09:12–09:59 UTC)\n- 12.4% of checkout attempts failed with 409/412\n- Cart page latency increased from p95 120ms to 410ms\n\n# Timeline\n- 09:12: Alerts: `redis_evicted_keys`rising\n- 09:16: checkout-service error rate spikes (409)\n- 09:22: Oncall enables CART_CACHE_BYPASS\n- 09:31: Redis memory stabilizes after key cleanup\n- 09:45: Gradual re-enable cache\n- 09:59: Metrics return to baseline\n\n# Root cause\nA new feature in cart-service stored expanded recommendation payloads inside cart cache values (including 20 recommended SKUs with metadata). Average cart cache value size increased ~6x, exceeding the expected memory budget.\n\nRedis config at the time:\n-`maxmemory 18gb`\n- `maxmemory-policy allkeys-lfu`\n\nLFU helped, but the increase in value size caused:\n- rapid evictions under load\n- increased misses -> more Postgres reads\n- ETag derived inconsistently when cache keys were evicted and rehydrated concurrently\n\n# Contributing factors\n- Missing alert on average Redis value size for `cart:*`keys\n- Load test did not include Black Friday recommendation payload shape\n- checkout-service treated 409 as a hard failure rather than retrying with refreshed ETag\n\n# Detection\nPrometheus alerts fired:\n-`increase(redis_evicted_keys_total[5m]) > 0`\n- `http_409_total`for checkout-service\n\n# Resolution\n- Enabled`CART_CACHE_BYPASS=true`to stop dependency on Redis\n- Purged oversized cart keys\n- Deployed hotfix to store only recommendation SKUs, not full metadata\n\n# What went well\n- Oncall had a documented bypass flag\n- Rollout of env changes was quick\n\n# What went poorly\n- Redis issue cascaded into checkout failures\n- Purging keys caused temporary DB pressure\n\n# Action items\n1. Enforce max cart cache value size (hard cap 128KB); if exceeded, skip caching.\n2. Update checkout-service to retry once on`CART_VERSION_MISMATCH`by refetching cart.\n3. Add dashboards:\n- average value size by key prefix\n4. Revisit ADR-2025-03 assumptions: Redis as primary cache for carts increases coupling.\n\n# Related\n- Runbook: RB-2026-02 Redis evictions\n- ADR: ADR-2025-03 Redis cart cache\n"
    },
    {
      "doc_id": "kb-2025-07-postmortem-migrations-caused-outage",
      "title": "Postmortem: Checkout Outage Due to Unapplied Migrations After Deploy",
      "document_type": "postmortem",
      "created_at": "2025-07-09",
      "content": "# Incident summary\nOn 2025-07-08, checkout-service deployed version 2.14.0 which expected a new column`orders.payment_provider`. The migration job did not run due to a misconfigured GitHub Actions environment variable, causing startup crashes and a partial outage.\n\n# Impact\n- Duration: 23 minutes\n- 100% of requests to `/api/checkout/start`returned 500\n- Other services mostly unaffected\n\n# Timeline\n- 14:02 UTC: Deploy initiated\n- 14:04: Pods crashloop with`UndefinedColumn`\n- 14:06: Oncall identifies missing migration\n- 14:10: Rollback executed\n- 14:18: Migration job run manually\n- 14:25: Redeploy successful\n\n# Root cause\nThe GitHub Actions workflow `promote.yml`sets`RUN_MIGRATIONS=true`for prod. A refactor renamed the variable to`RUN_DB_MIGRATIONS`in the migration cronjob chart, but the workflow was not updated. Migration did not execute.\n\nError observed:\n-`psycopg.errors.UndefinedColumn: column orders.payment_provider does not exist`\n\n# Contributing factors\n- No pre-deploy check verifying schema head\n- Rollout allowed new pods to replace old pods quickly, removing healthy capacity\n\n# Resolution\n- Rollback checkout-service\n- Manual migration job:\n```bash\nkubectl -n ecommerce-prod create job --from=cronjob/checkout-migrations checkout-migrations-manual\n```\n- Re-deploy after verifying alembic head\n\n# Action items\n1. Add deploy checklist item: verify `alembic current`equals expected head.\n2. Enforce`maxUnavailable=0`for checkout-service to prevent total outage during crashloops.\n3. Add alert when migration job did not run during a deploy window.\n\n# Related\n- Runbook: RB-2025-06 rollout stuck\n- Checklist: CL-2025-09 prod deploy\n"
    },
    {
      "doc_id": "kb-2025-12-postmortem-es-disk-watermark",
      "title": "Postmortem: Elasticsearch Disk Watermark Caused Search Outage",
      "document_type": "postmortem",
      "created_at": "2025-12-18",
      "content": "# Incident summary\nOn 2025-12-17, Elasticsearch cluster entered red state due to disk watermark thresholds on two data nodes. Shards became unassigned, and search-api returned 503 for a significant portion of requests.\n\n# Impact\n- Duration: 1h 12m\n- 38% of search requests failed (503)\n- Indexing lag grew to 4.6M messages\n\n# Timeline\n- 08:41 UTC: Disk usage crosses high watermark\n- 08:47: Cluster status turns yellow\n- 09:02: Cluster status turns red; unassigned primaries\n- 09:05: Oncall disables aggregations and scales indexing-worker to 0\n- 09:18: Old indices deleted to free space\n- 09:31: Cluster returns to yellow, then green\n- 09:53: Indexing resumes with smaller bulk size\n\n# Root cause\nILM policy`products-ilm-v3`failed to delete old indices because the policy referenced an outdated index pattern after an aliasing change. As a result, indices older than 14 days accumulated.\n\n# Contributing factors\n- No alert on ILM failures\n- Bulk indexing size was 1000, causing high segment merge pressure\n- Disk autoscaling was disabled for cost reasons\n\n# Resolution\n- Manually deleted old indices\n- Fixed ILM pattern\n- Reduced`INDEXING_BULK_SIZE`to 250 temporarily\n\n# Action items\n1. Add alert on ILM errors and on disk watermark thresholds.\n2. Add a weekly operational checklist item to verify retention.\n3. Validate ADR-2025-02 aliasing rollout steps include ILM pattern updates.\n\n# Related\n- Runbook: RB-2025-10 ES cluster red\n- ADR: ADR-2025-02 ES aliasing\n"
    },
    {
      "doc_id": "kb-2025-09-checklist-prod-deploy",
      "title": "Operational Checklist: Production Deploy (FastAPI + Node services)",
      "document_type": "checklist",
      "created_at": "2025-09-01",
      "content": "# Purpose\nA short checklist to reduce common production deploy failures across checkout-service, cart-service, search-api, and payment-gateway.\n\n# Before deploy\n- [ ] Confirm current incident status: no ongoing SEV-1/SEV-2.\n- [ ] Check Postgres connections < 70% of max.\n- [ ] Check Redis evictions flat.\n- [ ] Check Elasticsearch health green.\n- [ ] Verify feature flags for rollout are documented in the PR.\n\n# Build and artifacts\n- [ ] GitHub Actions:`ci.yml`green.\n- [ ] Image tag exists in registry and matches commit SHA.\n- [ ] For payment-gateway, confirm Node runtime stays at 18.x (do not auto-bump).\n\n# Database\n- [ ] If schema changes:\n  - [ ] Ensure migration cronjob exists and uses primary Postgres (not PgBouncer).\n  - [ ] Verify alembic head in staging.\n\n# Config and secrets\n- [ ] Secrets present in namespace:\n  -`STRIPE_API_KEY`, `STRIPE_WEBHOOK_SECRET`(payment-gateway)\n- [ ] ConfigMap changes reviewed (timeouts, pool sizes).\n\n# Deploy execution\n- [ ] Deploy to staging first; run smoke tests:\n  -`/health/ready`on all services\n  - Basic checkout flow (start -> pending)\n  - Search query returns results\n- [ ] Promote to prod.\n- [ ] Watch rollout status:\n```bash\nkubectl -n ecommerce-prod rollout status deploy/checkout-service\n```\n\n# After deploy (15 minutes)\n- [ ] Error rate stable (no sustained 5xx).\n- [ ] p95 latency within 20% of baseline.\n- [ ] Stripe webhook success rate stable.\n- [ ] Elasticsearch indexing lag not increasing.\n\n# Edge case notes\n- If enabling new Redis cache behavior, watch`redis_evicted_keys`and cart 409s.\n- If changing search mappings, verify alias`products_read`points to expected index.\n\n# References\n- RB-2025-06 rollout stuck\n- PM-2025-07 migrations caused outage\n"
    },
    {
      "doc_id": "kb-2024-10-checklist-weekly-ops",
      "title": "Operational Checklist: Weekly Platform Ops (Cache, Search, Payments)",
      "document_type": "checklist",
      "created_at": "2024-10-05",
      "content": "# Purpose\nWeekly checks to catch slow-burn issues (retention failures, creeping connection usage, webhook drift) before they become incidents.\n\n# Redis\n- [ ] Check memory usage trend and eviction count.\n- [ ] Sample key sizes for`cart:*`and`stripe:*`prefixes.\n- [ ] Verify`maxmemory-policy`remains`allkeys-lfu`.\n\n# Postgres\n- [ ] Check connection usage and top application_name consumers.\n- [ ] Check for `idle in transaction`sessions.\n- [ ] Review slow queries > 500ms from logs.\n\n# Elasticsearch\n- [ ] Verify cluster health green.\n- [ ] Verify ILM policy`products-ilm-v3`is active and not erroring.\n- [ ] Confirm retention: no`products-v3-*`older than 14 days unless explicitly needed.\n\n# Stripe / payments\n- [ ] Stripe webhook endpoint success rate over last 7 days.\n- [ ] Confirm`STRIPE_WEBHOOK_SECRET`unchanged and deployed consistently.\n- [ ] Check backlog metric`stripe_webhook_queue_depth`.\n\n# Kubernetes\n- [ ] Review CrashLoopBackOff counts.\n- [ ] Ensure resource requests/limits are not drifting (especially checkout-service).\n\n# Notes\nIf any check shows concerning trend (e.g., ES disk rising, Redis value sizes increasing), create an ops ticket and link to the relevant runbook.\n"
    }
  ]
}