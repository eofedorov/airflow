# P1. Prompt Engineering → Context Engineering
Описание: **“LLM-Gate — AI-шлюз для обработки входящих инженерных задач”**

---

# Цель проекта

Ты строишь сервис, который стоит **перед бэклогом команды** и автоматически:

1. Классифицирует входящие задачи
2. Извлекает структурированные данные
3. Возвращает строго валидный JSON
4. Не пропускает “грязный” или невалидный вывод

Нужно построить сервис, который:

* Принимает текст задачи (issue, баг-репорт, feature request)
* Возвращает **строго валидный JSON** по контракту
* Имеет библиотеку промптов с версиями
* Имеет автоматические тесты
* Логирует ошибки и невалидные ответы
* Умеет исправлять “почти валидный” ответ через repair-проход

---

# Пользовательский сценарий

Представь, что твоя команда получает 200 задач в неделю.

Нужно автоматически:

1 Классифицировать задачу

Пример входа:

> "После релиза 2.1.3 на странице оплаты появляется 500 ошибка.
> Шаги: добавить товар, перейти к оплате, выбрать карту."

Выход:

```json
{
  "label": "bug",
  "confidence": 0.93,
  "priority": "high",
  "requires_backend": true,
  "requires_frontend": false
}
```

---

2 Извлечь структурированные поля

Из текста задачи выделить:

* компоненты (checkout, payment)
* версии
* шаги воспроизведения
* тип влияния (loss of money / UX / cosmetic)

---


# Выбор стека (рекомендованный минимум)

* **Python + FastAPI**
* **Pydantic** для схем и валидации
* **pytest** для тестов
* (опционально) Docker — можно позже, в P1 не обязателен

# Структура репозитория (каркас)

```
p1-prompt-contracts/
  README.md
  pyproject.toml            # зависимости (poetry/uv/pip-tools — неважно)
  .env.example              # переменные окружения
  src/
    app/
      main.py               # FastAPI entrypoint
      settings.py           # конфиг (ключи, модель, лимиты)
      llm/
        client.py           # обертка над LLM API + ретраи/таймауты
        token_meter.py      # (опционально) учет токенов/стоимости
      prompts/
        registry.py         # реестр промптов (по имени)
        templates/
          classify_v1.txt   # шаблон промпта (Jinja/format)
          extract_v1.txt
        render.py           # рендеринг шаблона + сбор контекста
      contracts/
        schemas.py          # Pydantic-модели ответа (output contracts)
      services/
        runner.py           # “запуск задачи”: собрать контекст -> вызвать LLM -> валидировать -> repair при необходимости
      api/
        routes.py           # эндпоинты
  tests/
    test_render.py          # проверка рендера (без LLM)
    test_contracts.py       # валидация схем
    test_runner_mock.py     # runner с замоканным LLM
```

# Основные сущности (что именно реализовать)

## 1) PromptSpec (описание промпта)

Минимальные поля:

* `name` (строка): `classify`, `extract`
* `version` (строка/int): `v1`
* `template_path`: файл шаблона
* `system_rules`: короткие правила (например “Отвечай строго JSON по схеме …”)
* `output_schema`: ссылка на Pydantic-модель, которой должен соответствовать ответ

Идея: вы выбираете промпт по имени и версии, а всё остальное подтягивается из registry.

## 2) RenderContext (контекст для рендера)

Минимальные данные:

* `task` (описание задачи)
* `input` (текст/объект)
* `constraints` (например язык, стиль, запреты)
* `output_contract` (встроить краткое описание схемы: поля и типы)

Рендерер должен собрать финальные сообщения для LLM (system + user).

## 3) Output contracts (Pydantic схемы)

Сделайте 2-3 примера схем, чтобы отработать паттерн:

**(A) classify_v1**

* вход: произвольный текст
* выход (JSON):

  * `label`: строка из allowlist (например `bug|feature|question|other`)
  * `confidence`: float 0..1
  * `rationale`: короткая строка (1–2 предложения)

**(B) extract_v1**

* вход: текст описания вакансии/резюме/задачи
* выход:

  * `entities`: список объектов `{type, value}`
  * `summary`: строка до N символов

Важно: схемы должны быть **строгими** (forbid extra fields), чтобы модель не “расширяла” ответ.

## 4) Runner: вызвать LLM и гарантировать валидность

Логика:

1. Собрать messages (system + user)
2. Вызвать LLM
3. Попытаться распарсить JSON
4. Провалилось → сделать **repair attempt**:

   * второй вызов LLM с коротким промптом: “Преобразуй в валидный JSON по схеме, без пояснений”
5. Если после N попыток невалидно → вернуть ошибку с диагностикой

Это и есть практическая часть Context Engineering: вы не “надеетесь”, а **строите процесс**.

## 5) API

Минимум 2 эндпоинта:

* `POST /run/{prompt_name}`: принимает payload `{version, task, input, constraints}`
* `GET /prompts`: список доступных промптов и версий (из registry)

## 6) Тесты (обязательные для P1)

* Тест рендера: что шаблон подставляет параметры и не теряет важные блоки
* Тест контрактов: валидный пример проходит, невалидный — падает
* Тест runner: LLM замокан так, чтобы:

  * первый ответ был “почти JSON” → repair → валидный JSON
  * проверить, что repair действительно вызывается

---

# Итоговая картина системы

```
POST /run/classify_v1
POST /run/extract_v1

          ↓
Prompt Registry
          ↓
Context Builder
          ↓
LLM Client
          ↓
Schema Validator
          ↓
Repair (если нужно)
          ↓
Strict JSON
```

---

# Чёткие критерии “сдал / не сдал”

Проект считается завершённым, если выполнены ВСЕ пункты ниже.

---

## 1. Архитектура

* Есть registry промптов (с версиями)
* Промпт не захардкожен в коде
* Схема вывода отделена от логики вызова LLM
* Есть слой runner, а не “LLM вызывается прямо из API”

---

## 2. Контракт

* Ответ **всегда валидируется схемой**
* Лишние поля запрещены
* Если LLM вернул невалидный JSON:

  * система делает repair
  * или возвращает контролируемую ошибку

---

## 3. Тестирование

Есть тесты, которые:

* Проверяют, что рендер контекста работает
* Проверяют, что невалидный JSON падает
* Проверяют, что repair-проход реально вызывается
* Проверяют happy-path

---

## 4. Набор проверочных кейсов (Acceptance Tests)

Ты заранее создаёшь 10 входных задач.

Например:

* Баг
* Фича
* UX улучшение
* Дубликат
* Технический долг

Система должна:

* Правильно классифицировать минимум 8 из 10
* Не вернуть ни одного невалидного JSON

---

## 5. Логи и наблюдаемость (минимум)

В логах должно быть:

* Время ответа
* Использованная модель
* Количество токенов (если доступно)
* Был ли repair
* Ошибка парсинга (если была)

## P1. Prompt Engineering → Context Engineering

---


## Definition of Done для P1

* Есть минимум **2 промпта** в registry (с версиями)
* Есть минимум **2 строгие схемы** (Pydantic) и включена валидация
* `/run/{prompt}` возвращает **только валидный JSON** или понятную ошибку
* Есть тесты, которые покрывают: render + schema + repair path

